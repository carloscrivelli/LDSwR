[["index.html", "Learning Data Science with R Introduction", " Learning Data Science with R Dr Carlos Crivelli Associate Professsor/Reader in Affective Science and Social Interaction Division of Psychology (School of Applied Social Sciences) De Montfort University carlos.crivelli@dmu.ac.uk Introduction Learning Data Science with R is a book that aims to develop psychology students’ research skills relying on three main themes: Open Science, digital literacy, and science communication. The book covers Open Science principles, an introduction to R programming and data wrangling, descriptive statistics and data visualization, categorical data analysis, general and generalized linear models, and an introduction to psychometrics using R—a free software environment for statistical computing and graphics. Each chapter promotes the use of Open Science best practices to favor reproducibility and transparency at different stages of research data management process, along with learning key digital literacy skills and science communication competencies. Upon successful completion of this book, students should be able to: Recognize, explain, and apply Open Science best practices following the 2016 Concordat on Open Research Data and the UK Reproducibility Network guidelines. Identify and describe the appropriate analytical procedures required to answer research questions related to categorical data analysis, general and generalized linear models, and some psychometric models. Apply R to organize and tidy the data, to model the data, and to generate appropriate data visualizations. Inspect and appraise the outputs generated with R to produce with Rmarkdown a reproducible research report that will communicate the results relying on the principles of transparency and reproducibility in science. "],["how-to-use-this-book.html", "How to use this book", " How to use this book This book is intended to be a handbook used in psychology undergraduate (e.g., in 3rd-year optional modules) and postgraduate (e.g., MRes, MSc Psychological Research Methods) provisions interested in teaching data science with R for psychologists. Each chapter could be delivered in 1-2 hour sessions, with most of the content of the book aligned with a 13-week term delivery. It is recommended to install R, RStudio, and the required R packages to work on the examples of this book. Please, read the software installation guidelines before reading the book (Appendix A.1). "],["about-the-author.html", "About the Author", " About the Author Dr Carlos Crivelli is an Associate Professor/Reader in Affective Science and Social Interaction in the School of Applied Social Sciences at De Montfort University (Leicester, United Kingdom). After completing a BSc (Hons) Psychology, Dr Crivelli received his MSc and PhD in Methodology of Behavioral and Health Sciences from Universidad Autónoma de Madrid (Madrid, Spain). His research interests focus on three main areas: social influence and emotion science, cross-cultural psychology, and data science. He has published on these topics in journals like Trends in the Cognitive Sciences, Proceedings of the National Academy of Sciences, U.S.A., Perspectives on Psychological Science, and Current Directions in Psychological Science. Dr. Crivelli is an Honorary Fellow of the University of Melbourne (Australia), a Fellow of the Higher Education Academy, and a member of the editorial board of Nature–Scientific Data. He teaches undergraduate and postgraduate courses in Psychometrics and Data Science with R, Personality and Intelligence, and Conceptual Issues and Critical Debates in Psychology, embedding the intercultural dimension, and open and reproducible research into the curriculum. "],["what-is-open-science.html", "Chapter 1 What is Open Science?", " Chapter 1 What is Open Science? LEARNING OUTCOMES Identify and explain Open Science's principles. Compare the different initiatives promoting Open Science nationally and internationally. Appraise the implications of using Open Science's principles of reproducibility, openness, and replicability in the research process. Apply Open Science to comply with the regulations of research institutions and funders, governments, and publishers. Identify the steps required and implement the best practices to promote data sharing, open code and software, and data curation. According to the 2021 United Nations Educational, Scientific and Cultural Organization (UNESCO) Recommendation on Open Science, \"... Open Science is defined as an inclusive construct that combines various movements and practices aiming to make multilingual scientific knowledge openly available, accessible and reusable for everyone, to increase scientific collaborations and sharing of information for the benefits of science and society, and to open the processes of scientific knowledge creation, evaluation and communication to societal actors beyond the traditional scientific community. It comprises all scientific disciplines and aspects of scholarly practices, including basic and applied sciences, natural and social sciences and the humanities, and it builds on the following key pillars: open scientific knowledge, open science infrastructures, science communication open engagement o societal actors and open dialogue with other knowledge systems.\" (2017, p. 7) For The European Commission's task force on Research and Innovation (2021), \"Open Science is a system change allowing for better science through open and collaborative ways of producing and sharing knowledge and data, as early as possible in the research process, and for communicating and sharing results. This new approach affects research institutions and science practices by bringing about new ways of funding, evaluating, and rewarding researchers. Open Science increases the quality and impact of science by fostering reproducibility and interdisciplinarity. It makes science more efficient through better sharing of resources, more reliable through better verification and more responsive to society’s needs\" (p. 1) In the United Kingdom, different regulations on open science (also named Open Research or Open Scholarship) has been developed. For example, the 2019 Concordat to Support Research Integrity (Universities UK) or the 2016 Concordat on Open Research Data (HEFCE, Research Councils UK, Wellcome Trust, Universities UK) have captured some of the initiatives on Open Science discussed and developed by other policy makers, institutions, publishers, funders, and research councils. It is important to note that the UK Reproducibility Network---supported by UK Research Institute, the British Psychological Society, Wellcome Trust, Cancer Research UK, UK Data Service, or Universities UK---has played a pivotal role in \"seeking to understand the factors that contribute to poor research reproducibility and replicability, and to develop approaches to counter these and improve the quality of the research we produce\" (UK Reproducibility Network, 2021, p. 1). Open Science Open Science is a broad term used to encompass the promotion of transparency, reproducibility, research integrity, and societal impact (research and innovation). Because of those principles, Open Science can be reconceived as a meta-scientific movement interested in enhancing the following skills: Digital content creation Science communication Information and data literacy To empower researchers at every career stage, different organizations and Higher Education institutions have launched innovative educational programes along with institutional statements and strategies to promote Open Science best practices. For example, the University of Sheffield has launched an Open Science strategy to promote a culture of research excellence. The University of Manchester has created the Office for Open Research led big five strategic priorities: (1) open research skills, (2) open research communities, (3) open research recognition, (4) open research workflows, and (5) open and FAIR research outputs. Similarly, the University of Surrey developed a 5-year open research strategic goals and action plan that emphasizes the awareness, training and advocacy of open practice across the university in research, teaching, and learning. In December 2021, Loughborough University published an Open Research Position Statement to develop, review, and promote an institutional policy framework for Open Research. Echoing the University of Manchester's strategic priorities, Loughborough University has emphasized the pivotal role of training and support for PhD students and senior academics to update their skills for open data and open methods (i.e., science communication, information and data literacy). Other universities stressing the importance of mastering digital literacy skills for open data and code/software (e.g., programming in R/Python, data sharing) are the Open Research Skills Framework at the University of York, King’s College and their Open Research Group Initiative (KORGI), the University of Glasgow, the Reproducible Research Oxford group (RROx) at the University of Oxford, the Birmingham Environment for Academic Research's (BEAR) software carpentry training (in R, Python, Matlab, and Git) at the University of Birmingham, Open Research education for doctoral students at Imperial College London, or the Edinburgh Open Research Initiative of the University of Edinburgh. Table 1.1: Open Science Skills and Required Training Skills Training Digital content creation Copyright and intellectual property (CC-BY) Management and use of institutional repositories (DORA) OA publishers (e.g., bookdown) Open publication options (e.g., Gold, Green) Data repositories (OSF, Zotero, Figshare, Github) Data management plan Data presentation Science communication Bibliometrics, Altmetrics, and researcher impact DMU/Institutional webpage Personal brand (e.g., ORCID, Researchgate, Google scholar, Scopus) Public engagement (e.g., Leicester Business Festival) Research informs teaching (handbooks, monographs, tutorials) Innovation (e.g., the role of visual statistics during COVID-19) Information and data literacy Data analysis and visualization Data wrangling, modeling Text mining (qualitative research) Secondary sources (e.g., spatial data, census) Reproducibility and data reuse Publish in data journals Transparency Research integrity "],["the-credibility-of-science.html", "1.1 The credibility of Science", " 1.1 The credibility of Science The academic/research career runs in parallel with a cycle of credibility in which trust and reliability are sought (Miedema, 2022) (Figure 1.1). To conduct any research project, we need staff (at least, one researcher) and equipment. Then, we need to design a research proposal and collect data. The data needs to be properly analyzed, requiring a set of research skills that enabled researchers to design the research proposal first, to collect data ensuring proper levels of quality and rigor, and the necessary skills to describe, model, visualize, and interpret the findings. The findings are supported by arguments (some of them are theoretical, whereas others are methodological) and often presented as a manuscript or research report. However, to publish those manuscripts and research reports, researchers need to demonstrate appropriate and highly valued research skills (e.g., deep knowledge of the theoretical approaches and literature review, critical evaluation skills, understanding of the methods and data analytical techniques used), not only to create those products for internal consumption, but to be accepted by the scientific community after passing the judgment of senior and experts peers (the peer-review system). Once the products of our research are published, different metrics are used to provide the context of the quality of the research process. Metrics such as the Journal Citation Report impact factors, H-index, Google Scholar metrics, or Altmetrics are used for recognition and are a proxy to secure or increase the chances to secure funding, a good research/academic job, promotions, or to accrue more equipment and staff (e.g., research associates, post-docs, PhD students). Figure 1.1: The credibility cycle "],["questionable-scientific-practices.html", "1.2 Questionable scientific practices", " 1.2 Questionable scientific practices In the 2010s, disciplines such as psychology, economics, or the biomedical sciences were engulfed by a turmoil of distrust and lack of credibility. Although the Open Science movement is not new and it is at the core of the scientific method, a scientific reform movement emerged to inform on questionable scientific practices (John, Loewenstein, &amp; Prelec, 2012). Synchronically, a heated debate emerged to find solutions to the credibility crisis; a crisis that was triggered by the lack of openness, transparency, and replicability (Spellman et al., 2017). Questionable Scientific Practices Failing to report all dependent measures Collecting more data after seeing whether results were significant Failing to report all conditions Stopping collecting data earlier than planned because one found the result that one had been looking for Rounding down p values selectively reporting Selectively reporting studies that “worked” Excluding data after looking at the impact of doing so Reporting an unexpected finding as having been predicted from the start Falsely claiming that results are unaffected by demographic variables (e.g., gender) when one is unsure (or knows that they do) Falsifying data Figure 1.2: Prevalence of questionable research practices (John, Loewenstein, \\(\\&amp;\\) Prelec, 2012) "],["the-credibility-crisis.html", "1.3 The credibility crisis", " 1.3 The credibility crisis One of the most cited papers of the last decade (7,836 citations in August 2023) (Open Science Collaboration, 2015) conducted a large-scale study in which 100 classic psychological experiments were put to a test using an international, multi-lab approach. The main goal of the study was to estimate the reproducibility of psychological science. Unfortunately, the results showed that most of the classic psychological studies that psychology students learn in college and that most researchers cite as settled science couldn't be replicated. This meta-scientific endeavor has been conducted with similar results in other fields such as the behavioral, cognitive, economic, health, and medical sciences. The most telling results showed that: psychologists tend to use under-powered samples that overestimate the effect size and produce low reproducibility of results there is a lack of access to full methods there is a lack of access to analytic procedures and code the file-drawer problem is pervasive as editors and journals are keen to \"find results\" (i.e., rejection of the Null Hypothesis, rather that publish reports with null effects) there is a lack of access to publications there is discontent with reporting and use of standard statistics "],["threats-to-the-quality-of-the-scientific-process.html", "1.4 Threats to the quality of the scientific process", " 1.4 Threats to the quality of the scientific process Kovacs and colleagues (2021) conducted a research project to elucidate the origins and causes of the most frequent mistakes committed by researchers that damaged the quality of the scientific process. Using a mix of quantitative and qualitative approaches to data collection, they surveyed 488 researchers publishing in psychology journals between 2010 and 2018. They found four supra-ordinal categories or metagroups and their corresponding causes (Table 1.2). Table 1.2: Metagroups for Mistake Causes Metagroup Cause group Poor project preparation or management Bad or lack of planning Bad or lack of standards Bad skill management Miscommunication Failure to automate an error prone task Time management issue External difficulties High task complexity Technical issues Lack of knowledge Lack of knowledge/experience Personal difficulties Carelessness Inattention Lack of control Overconfidence Physical or cognitive constraints Note. Table reproduced from Kovacs, Hoekstra, and Aczel (2021). Figure 1.3 shows the mistakes reported by Kovacs et al. (2021) organized by the stages of research data management. To mitigate these mistakes different solutions have been proposed. For example, to define the data properly and to avoid ambiguous naming or to improve poor documentation practices, the use of codebooks, data management plans, and transparent research workflows is advisable (e.g., see Arslan, 2019). Similarly, to avoid wrong data processing and analysis, programming errors or loss of materials/data, using statistical code languages such as R or Python with embedded comments and storing the code in public repositories like Open Science Framework, Figshare, or GitHub is recommended (Klein et al., 2018). Figure 1.3: Threats to the quality of the scientific process (Kovacs, Hoekstra, \\(\\&amp;\\) Aczel, 2021) "],["open-data.html", "1.5 Open Data", " 1.5 Open Data Data sharing is a core element in Open Science to enhance the credibility and trust on our research practices. Data sharing allows for the reproducibility of previous findings, while enabling others to inspect and spot mistakes in the data, the meta-data, or the research workflow. Additionally, it prevents—to a certain extent—the emergence of scientific fraud due to fabricated data/results/visualizations or to the use of questionable data analytical practices. Data sharing also favors a swift progress in fields of research with difficulties in gaining access to relevant data (Gewin, 2016). It also enables researchers working at institutions based on countries with less resources and limited access to paywall journals and repositories. However, sharing data has its drawbacks. Some researchers invest years to collect data that is very difficult to obtain (e.g., long-term primate behavior in Kibale National Park, Uganda) with the idea of completing a research project that will produce several outputs. If the data is shared in the first publication, sharing might be a problem for future publications (Hunt, 2019). To partially mitigate this problem, many journals allow researchers to publish the data sets in specialized data journals such as Nature—Scientific Data, GigaScience, BMC Research Notes, or Data in brief. For example, in Nature—Scientific Data, these data descriptors are curated and can be modified to incorporate new data collected later. Moreover, Nature-titled journals do not consider prior Data Descriptor publications to compromise the novelty of new manuscript submissions if those manuscripts go substantially beyond a descriptive analysis of the data, and report important new scientific findings appropriate for the journal in question (Nature—Scientific Data, 2023). Similarly, sharing data is not as easy and straightforward as it seems (e.g., to upload my anonymized csv file into a public repository). Researchers—especially postgraduate students and early career researchers-—must learn a new set of skills in order to publish their research products: Data curation Data management plan Storing, saving, archiving, and data preservation Meta-data Data analysis and visualization Data wrangling Reproducibility and data reuse Compliance with FAIR data principles "],["fair-data.html", "1.6 FAIR Data", " 1.6 FAIR Data In 2016, the data journal Nature—Scientific Data published the FAIR Guiding Principles for Scientific Data Management and Stewardship to clarify and guide researchers with the FAIR principles: Findable, Accessible, Interoperable, Reusable. The FAIR principles pivot around the idea of a continuum of \"machine-actionability\" in which machines can act autonomously on data objects, (1) in relation to the contextual metadata surrounding a digital object (what is it?), and (2) when referring to the content (how do I process it/integrate it?) (Wilkinson, 2016). Table 1.3: The FAIR Guiding Principles Principles Guidelines Findable F1. (meta)data are assigned a globally unique and persistent identifier F2. data are described with rich metadata (defined by R1 below) F3. metadata clearly and explicitly include the identifier of the data it describes F4. (meta)data are registered or indexed in a searchable resource Accessible A1. (meta)data are retrievable by their identifier using a standardized communications protocol A1.1 the protocol is open, free, and universally implementable A1.2 the protocol allows for an authentication and authorization procedure, where necessary A2. metadata are accessible, even when the data are no longer available Interoperable I1. (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation I2. (meta)data use vocabularies that follow FAIR principles I3. (meta)data include qualified references to other (meta)data Reusable R1. meta(data) are richly described with a plurality of accurate and relevant attributes R1.1. (meta)data are released with a clear and accessible data usage license R1.2. (meta)data are associated with detailed provenance R1.3. (meta)data meet domain-relevant community standards "],["contributorship-credit.html", "1.7 Contributorship (CRediT)", " 1.7 Contributorship (CRediT) The Contributor Roles Taxonomy (CRediT) has been designed to cover 14 key roles representing the spectrum of activities involved in the production of research outputs. Most of the big publishers (e.g., Nature, Elsevier, SAGE, Cell Press, Wiley) and psychological societies (American Psychological Association [APA]), Association for Psychological Science [APS]) have made the CRediT system compulsory. The Contributor Roles Taxonomy (CRediT) Conceptualization Data curation Formal analysis Funding acquisition Investigation Methodology Project administration Resources Software Supervision Validation Visualization Writing—original draft Writing—review and editing Some of the desired consequences of this contributor system are to avoid: Ghost authorship: Authors who contributed to the work but are not listed, generally to hide a conflict of interest from editors, reviewers, and readers. Gift authorship: Individuals given authorship credit who have not contributed in any substantive way to the research but are added to the author list by virtue of their stature in the organization. Orphan authorship: Authors who contributed materially to the work but are omitted from the author list unfairly by the drafting team. Forged authorship: Unwitting authors who had no part in the work but whose names are appended to the paper without their knowledge to increase the likelihood of publication. "],["references.html", "1.8 References", " 1.8 References Arslan, R. C. (2019). How to automatically document data with the codebook package to facilitate data reuse. Advances in Methods and Practices in Psychological Science, 2(2), 169—187. European Commission. (2021). Open Science [Fact sheet]. European Union. Gewin, V. (2016). Data sharing: An open mind on open data. Nature, 529, 117—119. Hunt, L. T. (2019). The life-changing magic of sharing your data. Nature Human Behavior, 3, 312—315. John, L. K., Loewenstein, G., &amp; Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23(5), 524—532. Klein O., Hardwicke T. E., Aust F., Breuer J., Danielsson H., Mohr A. H., Ijzerman H., Nilsonne G., Vanpaemel W., Frank M. C. (2018). A practical guide for transparency in psychological science. Collabra: Psychology, 4(1):20. Kovacs, M., Hoekstra, R., &amp; Aczel, B. (2021). The role of human fallibility in psychological research: A survey of mistakes in data management. Advances in Methods and Practices in Psychological Science, 4(4), 1—13. Miedema, F. (2022). Open Science: The very idea. Springer. Nature—Scientific Data (2023). Frequently Asked Questions. Retrieved from Nature—Scientific Data website: https://www.nature.com/sdata/faq Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), 1—8. Spellman, B., Gilbert, E. A., &amp; Corker, K. S. (2017, September 20). Open Science: What, Why, and How. https://psyarxiv.com/ak6jr/ UK Reproducibility Network. (2021, September). Terms of Reference. Version 3.3. United Nations Educational, Scientific, and Cultural Organization. (2021, November). UNESCO Recommendation on Open Science (Programme Document SC-PCB-SPP/2021/OS/UROS). Retrieved from the United Nations Educational, Scientific, and Cultural Organization website: https://unesdoc.unesco.org/ark:/48223/pf0000379949 Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J. W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., Gonzalez-Beltran, A., … Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3:160018. "],["introduction-to-r.html", "Chapter 2 Introduction to R", " Chapter 2 Introduction to R LEARNING OUTCOMES Identify the capabilities of R and RStudio's environment and appraise their functionality. Distinguish between R functions, objects, and diverse data wrangling approaches. Apply basic programming skills to import and organize the data using classic data wrangling approaches and the tidyverse grammar. Evaluate the R code and appraise the outputs to demonstrate a satisfactory level of basic programming skills in R. R is a programming language and an open-source software environment for statistical computing and graphics available for Windows, MacOS, and Linux operating systems (R Core Team, 2023). R is an interactive environment for data science that enables us to import, manipulate, model, and visualize data to effectively communicate our results (Wickham &amp; Grolemund, 2017). The use of R also promotes transparency and open research, with a clear focus on supporting reproducibility policies (Gandrud, 2015). R, in its most basic form, is composed by (1) a script in which we write the lines of R code that we run, and (2) the console that displays the results and debugging (Figure 2.1). If we generate a plot, it will emerge as a pop-up window, showing one plot at a time. For more details on what is R, click here. Figure 2.1: R displayed in two panels: Script (left) and Console (right). Why Use R? It is a free and open-access programming environment for data science Reproducible and transparent research in psychological science Unlimited capabilities for data science (e.g., analysis, modelling, visualization) It is dynamic. R is constantly evolving. It provides state-of-the-art solutions for any imaginable field (e.g., economics, psychology, genetics, medicine, ecology, sociology, computational physics, geography, psychometrics) R has the biggest global community providing all sorts of free resources to support learners (e.g., reference manuals, books, on-line tutorials, R code and notebooks, cheat sheets, Internet forums) R is more efficient than any other commercial software UK universities teaching 21st century data science with R: University of Glasgow, Lancaster University, Nottingham Trent University, University of Edinburgh, University of Lincoln, University of Manchester, University of Plymouth, University of Sussex, Queen Mary University of London, University of Sheffield, University of Essex, Oxford University, King's College, University of Warwick... Employability "],["what-is-rstudio.html", "2.1 What is RStudio?", " 2.1 What is RStudio? RStudio is an Integrated Development Environment (IDE) for R. RStudio facilitates the task of coding by providing an enhanced programming experience. First, RStudio is free and easy to learn. Second, RStudio's elements are displayed in a four-panel display: the code editor (the script), the console (the results and debugging), the global environment (e.g., R objects, history of our session), and a notebook that includes different active tabs (e.g., plots, packages, help) (Figure 2.2). Last, there is an optimal integration of the different panels and tabs to run R sessions and projects smoothly. Figure 2.2: RStudio's four-panel display. Why Use RStudio? RStudio's main components are integrated into a four-pane layout: the code editor (the script), the console (the results and debugging), the global environment (the R objects, history of our session), and a notebook to hold tabs for the files/plots/packages/help/viewer The code editor is feature-rich and integrated with the built-in console The code editor and console are efficiently linked to the files/plots/packages/help/viewer panels RStudio is available for Windows, MacOS, and Linux RStudio is easy to learn For more details on what is RStudio, click here "],["lets-get-started.html", "2.2 Let's get started!", " 2.2 Let's get started! In the first place, R and RStudio should be installed in your machine. To learn how to install R and RStudio in your computer, please read carefully Appendix A.1. 2.2.1 Running a line of code There are several ways to run a line of code: Highlight the line of code and click on the button Run (\\(\\Rightarrow\\) Run) displayed on the upper right side of the code editor. Highlight the line of code that you want to run and press Ctrl + Enter (Windows) or Cmd + Enter (MacOS). Place the cursor on the line of code and click on the button Run (\\(\\Rightarrow\\) Run). Press Ctrl + Shift + S (Windows) or Cmd + Shift + S (MacOS) to run the code of an entire script. Type a line of code directly into the console or copy a line of code from the code editor and paste it into the console after the prompt (&gt;). Then, press Enter on the console. 2.2.2 Adding comments on R scripts It is useful to add comments before chunks of R code to organize the session and to inform others that might be interested in reproducing our session and evaluating our outputs. The hash mark (#) is used to make comments by placing the symbol at the beginning of a line of code. R won't execute the line of code that is preceded by one or more hash mark symbols. # This is a comment # Raise 5 to the power of 2 5 ^ 2 ## [1] 25 sqrt(25) # sqrt(x) returns the square root of x ## [1] 5 "],["starting-a-session-in-rstudio.html", "2.3 Starting a session in RStudio", " 2.3 Starting a session in RStudio 2.3.1 Setting the global options It is possible to customize RStudio (e.g., code, appearance, organization of panels) to adapt it to our needs and preferences. To do so, click on the main RStudio menu bar \\(\\rightarrow\\) Tools \\(\\rightarrow\\) Global Options. Although there are many available options to customize RStudio sessions, we will mention below some of the most interesting features to inspect in the global options settings: General: It is convenient to start a new R session to avoid carrying over variable corruptions. Consequently, do not restore or save .RData files into the workspace at startup. Code-Display options: To enhance the usability experience, enable soft-wrap R source files, highlight selected word, show line numbers, and blinking cursor. In the option show margin, set the margin column to 60. Appearance: The customization of the editor and console's appearance (e.g., font size) is pivotal as we will spend a lot of time looking at the computer screen when programming. For example, the use of dark background colors (e.g., material, chrome) is very popular among programmers because it minimizes eye strain or fatigue. If you are a low vision user, the high contrast generated by a dark background and white text will be beneficial as well. Pane Layout: It is possible to reorganize the location of the panels. Likewise, it is possible to show or hide the different tabs available in the global environment and notebook. For instance, we could arrange the source on the upper left-hand side, inspecting the output of the code that we run underneath or on the right hand side (the console would be located on the bottom left corner or on the upper right corner respectively). 2.3.2 First lines of code Every session will start with three lines of code that we will include at the beginning of our R script. First, we will use the function rm() (i.e., remove) to delete any R object stored in the global environment. We want to start each session anew, without carrying over R objects from previous sessions. Second, we will set the randomization seed with the function set.seed() to reproduce the R objects and outputs of any simulated data and computation. Last, we will turn off the scientific notation used in R. rm(list = ls()) set.seed(1234) options(scipen=999) 2.3.3 Setting the working directory There is a point-and-click approach to set the working directory for each R session. To do so, click on the main RStudio menu bar \\(\\rightarrow\\) Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory. Then, select the desired folder to store all the files of the session (e.g., the R file with the code, data sets). When exporting plots and files, these files will be saved and stored locally in the working directory already set at the beginning of the session. For example, we created a folder called R located in the main SSD MacOS/C: hard drive that includes the directories of all our R projects. At a lower level, we created another folder called psyc3012 (Data Science with R) to store the current R project that, in turn, includes one folder per session (e.g., session1, session2, session3). Setting the path for the working directory The path should be short Create a folder called R in your PC (C:) or MacOS (HD/SSD) hard drives. Then, create folders for different projects in the R folder Use short names with no spaces (e.g., do not use a folder name such as session 1, but session1 or session_1) R is case sensitive (psyc3012 differs from PSYC3012) Use single ('x') or double quotes (\"x\") because the path is a character string The function getwd() finds the path to the folder of our current working directory, whereas the function setwd() sets the working directory's path. getwd() setwd(&#39;~/R/psyc3012/session1&#39;) "],["r-objects-and-functions.html", "2.4 R objects and functions", " 2.4 R objects and functions There are different kinds of entities that we identify as objects. Here, we will distinguish between R objects of different data structures (e.g., matrices, vectors, data frames, lists) and \"recursive\" objects such as those of mode function (Lander, 2017; Venables et al., 2009). We will refer to the latter as functions and to the former as R objects. In sum, we will use functions (e.g., mean(), boxplot()) to execute commands on R objects (e.g., atomic vectors, data frames, matrices). 2.4.1 Functions Let's imagine that we would like to command a robot to do something for us. We simply need to learn the robot's language (i.e., rules of grammar, syntax, vocabulary, code), so it can decode our commands and execute our orders. Any programming language will work in a similar way. For instance, we will need to read data from files (e.g., text file, CSV file, SPSS file) or to simulate our own data. These data should be appropriately stored and saved into R's global environment to be able to manipulate and organize our data (e.g., merge, subset, select, rename), to model and visualize the data, and to publish the results. To complete those tasks, we must use appropriate functions on data structures of the same mode. 2.4.1.1 The c() function The c() function is usually referred as the concatenate or combine function. It combines two or more elements of the same mode (e.g., numerical values, character strings, logical values) to create vectors. c(16, 71, 172, 53, 77) ## [1] 16 71 172 53 77 c(&#39;Hylobates&#39;, &#39;Pongo&#39;, &#39;Gorilla&#39;, &#39;Pan&#39;, &#39;Homo&#39;) ## [1] &quot;Hylobates&quot; &quot;Pongo&quot; &quot;Gorilla&quot; &quot;Pan&quot; &quot;Homo&quot; c(FALSE, TRUE, TRUE, TRUE, TRUE) ## [1] FALSE TRUE TRUE TRUE TRUE 2.4.2 R objects R objects such as character strings and numeric vectors, data frames, matrices or lists are saved and stored in R's global environment when we assign them to a name using the assignment operator (name \\(\\gets\\) data). The console shows the object only if it is recalled. To produce the assignment operator in Windows/Linux operating systems press Alt + -. Alternatively, press Option + - if using MacOS. As a computing programming language, R executes our commands to produce certain outcomes. Sometimes those outcomes will appear on the console (e.g., estimating the arithmetic mean on a numerical vector). Other times, however, these commands won't produce effects on the console. For example, when we create a vector, it will only appear in the global environment tab unless it will be recalled in the same line of code preceded by a semicolon (;) or plaxed on a line of code below. weight &lt;- c(16, 71, 172, 53, 77); weight ## [1] 16 71 172 53 77 apes &lt;- c(&#39;Hylobates&#39;, &#39;Pongo&#39;, &#39;Gorilla&#39;, &#39;Pan&#39;, &#39;Homo&#39;) apes ## [1] &quot;Hylobates&quot; &quot;Pongo&quot; &quot;Gorilla&quot; &quot;Pan&quot; &quot;Homo&quot; If we don't assign a set of values to any given name to create an R object, these values won't be saved into the global environment. Therefore, we won't be able to recall the object for future computations. c(16, 71, 172, 53, 77) ## [1] 16 71 172 53 77 Instead of creating an R object first, we could be tempted to paste a vector into a function. This approach is not efficient---specially with large data structures---and it should be avoided. Given the outstanding possibilities that R offers, we should create R objects and use functions on them to model and visualize the data. mean(c(16, 71, 172, 53, 77)) ## [1] 77.8 weight.apes &lt;- c(16, 71, 172, 53, 77) mean(weight.apes) ## [1] 77.8 mean(weight.apes, trim = .20) ## [1] 67 median(weight.apes) ## [1] 71 sd(weight.apes) ## [1] 57.78148 length(weight.apes) ## [1] 5 barplot(weight.apes, ylab = &#39;Weight (Kg)&#39;, xlab = &#39;Apes&#39;, col = &#39;coral3&#39;, ylim = c(0, 200), names.arg = c(&#39;Hylobates&#39;, &#39;Pongo&#39;, &#39;Gorilla&#39;, &#39;Pan&#39;, &#39;Homo&#39;)) Naming objects R is case sensitive (e.g., the R object weight differs from Weight) Do not start with a number or special characters (e.g., +, -, &amp;, *, ?) Using lower (e.g., weight.apes, x, m2spr34) rather than an upper case letters (e.g., Weight.Apes, X, M2spr34) facilitates the programming flow Do not use names of functions to name R objects (e.g., data(), mean(), plot(), factor()) When using several words, do not leave spaces. Alternatively, use the full stop, underscore, or a combination of UPPER and lower case letters (e.g., g.spatial.males, g_fluid_females, gCrystallized) Use short, but informative names (e.g., g.spatial.m, g_fluid_f, gCrystal) 2.4.3 Applying functions to objects Functions work on data stored in R objects and saved in the global environment. We will place the name of the R object within the parentheses of functions, executing the command of the function on R objects. Below, we will compute the arithmetic mean (M = 52.1) and estimate the length (N = 10) of one numeric vector named age. age &lt;- c(37, 19, 74, 62, 51, 56, 58, 76, 45, 43) mean(age) ## [1] 52.1 length(age) ## [1] 10 Functions usually include arguments that allow us to modify their default settings. For example, the function mean() computes the arithmetic mean on the R object placed within parentheses. In the previous example, some outliers (e.g., 19, 76) might be distorting the estimation of the age distribution's central tendency. Luckily, the argument trim can be set from 0 (no trimming) to 0.5 (maximum trimming) to compute robust estimators of central tendency. By trimming 20% of the observations from each tail of the distribution (trim = .20), we will compute 20% trimmed means---a robust estimator of central tendency (Wilcox, 2016). The maximum trimming (trim = .50) computes the median as it only considers the central value of the distribution. mean(age, trim = 0) ## [1] 52.1 mean(age, trim = .20) ## [1] 52.5 mean(age, trim = .50) ## [1] 53.5 median(age) ## [1] 53.5 2.4.4 Getting help All R functions are included in packages that are created and maintained by their authors (e.g., statisticians, psychologists, biologists, sociologists). R packages are freely available on the Internet at the Comprehensive R Archive Network or CRAN. Every package has its own documentation that describes and explains how to use the functions of each package. We can also access to cheat sheets, on-line learning tools, forums and on-line communities, blogs, or even open-access books created in R using bookdown (Xie, 2017; Xie, Allaire, &amp; Grolemund, 2019). RStudio includes the documentation of functions and packages in the Help tab. For example, if we search for the function mean(), we will find the documentation for this function included in R's package base. Another way to access to the documentation of packages and functions when help is required is to use the function help() or the question mark (?) followed by the name of the function. help(mean) ?mean "],["packages.html", "2.5 Packages", " 2.5 Packages A package is a collection of functions, data sets, and compiled R code for a given purpose (e.g., visualize data, perform specific data analytical techniques). First, we must install the packages on our computer. These packages will be downloaded and stored locally just once. Then, we will upload these packages into our R session when required. The function .libPaths() shows the directory in which our packages are installed. .libPaths() R comes with a standard set of packages (e.g., base, stats). However, most of the time we will use specific packages that include useful functions to organize, model, and visualize our data. The function library() is used to inspect the packages that we have previously installed on our computer. library() Currently (1st May 2023), there are 19,486 different R packages available on CRAN's (i.e., the Comprehensive R Archive Network) repository. These packages are grouped into 42 topics (e.g., psychometrics, graphics, genetics, social sciences, chemometrics and computational physics, econometrics, machine learning, finance, clinical trials, hydrology, medical image analysis, environmetrics, reproducible research). Some packages' names are self-explanatory (e.g., sem for Structural Equation Models, apaTables for generating APA formatted Tables, Kendall for computing Kendall's nonparametric correlation tests), whereas other names include the abbreviation of specific tests, statistical techniques, or even research methodologies (e.g., WRS2 for Wilcoxon's robust statistics, lavaan for latent variable analysis, sna for social network analysis). To learn how to install R packages in your machine, please read carefully Appendix A.2. To inspect the list of R packages used on this book and that you will need to install to reproduce all the examples of the book, please read Appendix A.3. 2.5.1 Loading R packages Some basic packages (e.g., base, stats, graphics) are automatically uploaded when we start our session in R and RStudio. In contrast, most R packages will have to be uploaded at the beginning of our session in order to use them. The functions library() and require() load packages, using the name of the package as the first argument of the function. When the packages are successfully uploaded into our session, they are displayed with a ticked box (i.e., active) in the packages tab. library(magrittr) require(dplyr) "],["importing-and-exporting-data.html", "2.6 Importing and exporting data", " 2.6 Importing and exporting data The package rio enables us to import and export data sets from and into different formats (e.g., text, CSV, Excel, SPSS files). Traditionally, R users relied on functions such as read.table(), read.csv(), read_excel(), or read_sav() to import text files, csv comma-separated files, Excel files, or SPSS files. Similarly, the functions write.table(), write.csv(), write_xlsx(), and write_sav() were used to export data sets created and stored in the R session into an array of different formats. Unfortunately, to use most of these functions, R packages such as haven, readxl, or writexl were required. Luckily, the package rio operates like a Swiss-army knife, allowing us to easily import and export data via generic functions whose arguments specify the type of file, the path, the name, etc. 2.6.1 Loading data For example, we could read built-in data from the data sets loaded in our session using the function data(). From all the files available, we will read the data set women which includes the height and weight of 15 women. data() data(women) women ## height weight ## 1 58 115 ## 2 59 117 ## 3 60 120 ## 4 61 123 ## 5 62 126 ## 6 63 129 ## 7 64 132 ## 8 65 135 ## 9 66 139 ## 10 67 142 ## 11 68 146 ## 12 69 150 ## 13 70 154 ## 14 71 159 ## 15 72 164 Now, we will use the generic function import() from the package rio to read the burnout data set. The function head() shows the first six observations/participants of the R object included as the first argument of the function. In the following example, we are modifying the default settings that show the data of the first six observations/participants. We will request the first eight observations of the R object burnout. my.burnout &lt;- rio::import(&#39;./datasets/burnout.sav&#39;) head(my.burnout, 8) ## id sex age i1 i2 i3 i4 i5 i6 i7 i8 i9 i10 i11 i12 burnoutTot stress job.satis ## 1 1 0 34 4 3 4 2 3 4 3 4 4 4 4 4 34 88 48 ## 2 2 0 38 2 2 4 3 4 2 3 2 3 3 4 3 28 79 53 ## 3 3 1 43 2 5 5 2 4 2 2 5 2 3 3 1 22 82 68 ## 4 4 1 37 3 3 2 4 4 3 3 2 3 4 3 3 33 89 54 ## 5 5 0 43 2 2 4 3 3 1 3 4 3 2 4 2 23 76 51 ## 6 6 1 35 3 3 4 3 3 3 3 3 3 3 4 2 29 83 47 ## 7 7 1 31 2 4 1 4 4 4 4 2 4 2 5 3 37 86 59 ## 8 8 1 25 3 4 1 4 3 2 4 2 4 5 3 4 38 84 50 ## coping.skills contract ## 1 71 2 ## 2 82 2 ## 3 56 3 ## 4 64 1 ## 5 73 3 ## 6 74 3 ## 7 75 1 ## 8 80 2 CAUTION! The data set that we want to import should be stored in the same folder of our working directory. If it is located in a different folder, the path should be specified in the code. For example, if the data set is in a folder named data inside of our working directory, the code should be as follows: my.burnout &lt;- rio::import(&#39;./data/burnout.sav&#39;) IBM SPSS stores data frames into .sav files. Consequently, the file extension of the data set that we want to import needs to be explicitly included in the name of the file. To read CSV comma-delimited files, we will have to include the extension .csv. For Microsoft Excel files (i.e., spreadsheets) storing arrays of data sets using different sheets, we will use the file extension .xlsx. Using two colons (::) to load specific functions We can use functions from packages that we have not previously loaded by adding the name of the package followed by two colons before the specific function we intend to use [e.g., rio::import(), psych::desbribeBy()]. 2.6.2 Exporting data To export data from our R session, enter the R object (e.g., a data frame) as the first argument and the appropriate file extension as the second argument (e.g., format = 'txt'). Below, we will export the data set called my.burnout as an Excel file (i.e., format = 'xlsx'). rio::export(my.burnout, format = &#39;xlsx&#39;) "],["references-1.html", "2.7 References", " 2.7 References Gandrud, C. (2015). Reproducible Research with R and RStudio (2nd ed.). Chapman and Hall/CRC. Lander, J. P. (2017). R for everyone (2nd ed.). Addison-Wesley. R Core Team (2023). R: A language and environment for statistical computing. R Foundation for Statistical Computing. Venables, W. N., Smith, D. M., &amp; the R Development Core Team. (2009). An Introduction to R. Network Theory Limited. Wickham, H., &amp; Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data. O'Reilly. Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., Takahashi, K., Vaughan, D., Wilke, C., Woo, K., &amp; Yutani, H. (2019). Welcome to the Tidyverse. Journal of Open Source Software, 4(43), 1686. Retrieved at https://joss.theoj.org/papers/10.21105/joss.01686 Wickham, H., François, R., Henry, L., &amp; Müller, K. (2022). dplyr: A grammar of data manipulation. Retrieved at https://dplyr.tidyverse.org Xie, Y. (2015). Dynamic documents with R and knitr (2nd ed.). Chapman and Hall/CRC. Xie, Y., Allaire, J. J., &amp; Grolemund, G. (2019). R Markdown: The definite guide. Chapman and Hall/CRC. "],["data-wrangling.html", "Chapter 3 Data Wrangling", " Chapter 3 Data Wrangling LEARNING OUTCOMES Identify the capabilities of R and RStudio's environment and appraise their functionality. Distinguish between R functions, objects, and diverse data wrangling approaches. Apply basic programming skills to import and organize the data using classic data wrangling approaches and the tidyverse grammar. Evaluate the R code and appraise the outputs to demonstrate a satisfactory level of basic programming skills in R. We are going to create a data frame by simulating a data set. Data frames are data structures that include vector columns (i.e., variables) containing different modes of data (e.g., numeric, factor, character). The observations, participants, or cases are usually displayed as rows. Using the assignment operator (&lt;-) we will store the data that we will simulate into R objects that will be named accordingly to the variables they intend to simulate (e.g., id, treatment, life.satis). id &lt;- 1:36 treatment &lt;- rep(1:3, each = 12) year &lt;- rep(2020, times = 36) sex &lt;- rep(c(&#39;male&#39;, &#39;female&#39;), times = 18) depression &lt;- c(22, 19, 14, 23, 16, 11, 29, 19, 17, 14, 20, 22, 11, 8, 5, 7, 13, 10, 9, 7, 16, 13, 11, 14, 8, 12, 7, 4, 9, 7, 11, 3, 5, 9, 14, 4) life.satis &lt;- rnorm(36, mean = 40, sd = 12.3) alone &lt;- rbinom(36, 1, .29) The function data.frame() allows us to include different column vectors as arguments. The function names() shows the variables' names. affective.dis &lt;- data.frame(id, treatment, year, sex, depression, life.satis, alone) head(affective.dis, 8) ## id treatment year sex depression life.satis alone ## 1 1 1 2020 male 22 53.50930 0 ## 2 2 1 2020 female 19 42.10586 0 ## 3 3 1 2020 male 14 22.73856 1 ## 4 4 1 2020 female 23 18.42421 0 ## 5 5 1 2020 male 16 68.70935 1 ## 6 6 1 2020 female 11 40.22030 1 ## 7 7 1 2020 male 29 32.00596 0 ## 8 8 1 2020 female 19 44.29188 1 names(affective.dis) ## [1] &quot;id&quot; &quot;treatment&quot; &quot;year&quot; &quot;sex&quot; &quot;depression&quot; ## [6] &quot;life.satis&quot; &quot;alone&quot; In the following sections, we will apply basic data wrangling functions to reorganize and tidy the data frame called affective.dis (i.e., affective disorders); a data set that includes the vector columns that we have recently simulated and inspected: id, treatment, year, sex, depression, life.satis, and alone. "],["indexing-and-subsetting.html", "3.1 Indexing and subsetting", " 3.1 Indexing and subsetting We can use numeric indexes and the function subset() in order to extract parts of a vector, matrix, or data frame, generating a different version of the original data set. For instance, we could be interested in analyzing the psychometric properties of an ability test, focusing only on the specific items of a scale. Likewise, we could be interested in plotting and modeling the data of a subgroup of participants. Sometimes, it is recommended to rename variables or to transform them into new ones by recoding them. In all these operations, we need to specify which rows (observations, cases, participants) and columns (the variables of interest) will be selected, deleted, transformed, renamed, or recoded. 3.1.1 Selecting columns (variables) There are different ways to select one vector column or variable. We can use the dollar symbol ($) between the data set (placed before the dollar symbol) and the name of the variable that we want to select (placed after the dollar symbol). We can use square brackets after the data set's name. Inside of the brackets we can either include the numerical value showing in which location that particular variable is placed in the data set (e.g., a number 4 will mean that we want to select the fourth variable from the data set) or the name of the variable within quotation marks (e.g., 'sex'). We can use the classic indexing approach to matrices with square brackets. We will select all rows (i.e., in the first dimension or rows, we will leave a blank space before the comma) and the specific vector column after the comma (e.g., in the second dimension or columns, a 4 will select the data from the fourth column; i.e., the variable sex). affective.dis$sex affective.dis[4] affective.dis[&#39;sex&#39;] affective.dis[ , 4] Sometimes, we are interested in selecting more than one variable. To do so, we can use the function c() with the indexes or names of the variables. To drop one or more variables we can use the minus sign before the indexes. head(affective.dis[ , c(1, 2, 4, 5, 7)]) ## id treatment sex depression alone ## 1 1 1 male 22 0 ## 2 2 1 female 19 0 ## 3 3 1 male 14 1 ## 4 4 1 female 23 0 ## 5 5 1 male 16 1 ## 6 6 1 female 11 1 head(affective.dis[ , -7]) ## id treatment year sex depression life.satis ## 1 1 1 2020 male 22 53.50930 ## 2 2 1 2020 female 19 42.10586 ## 3 3 1 2020 male 14 22.73856 ## 4 4 1 2020 female 23 18.42421 ## 5 5 1 2020 male 16 68.70935 ## 6 6 1 2020 female 11 40.22030 3.1.2 Selecting rows (observations) To select observations, cases, or participants (i.e., rows) we will use the approach for indexing matrices. This time, the blank space will be left after the comma (i.e., columns) and we will select the observations by indexing the space before the comma (i.e., rows). affective.dis[12:15, ] ## id treatment year sex depression life.satis alone ## 12 12 1 2020 female 22 48.86415 0 ## 13 13 2 2020 male 11 31.99603 0 ## 14 14 2 2020 female 8 29.28087 0 ## 15 15 2 2020 male 5 64.95737 0 affective.dis[c(1, 18, 36), ] ## id treatment year sex depression life.satis alone ## 1 1 1 2020 male 22 53.50930 0 ## 18 18 2 2020 female 10 31.53965 0 ## 36 36 3 2020 female 4 41.11086 0 3.1.3 Selecting rows and columns (conditional subsetting) Although we could use a matrix indexing procedure to select both observations and variables, the use of logical operators is a more efficient approach. For instance, we could be interested in selecting a subset of subjects that were told to exercise daily to treat their depression (treatment = 1). affective.dis[affective.dis$treatment == 1, ] affective.dis[affective.dis[&#39;treatment&#39;] == 1, ] We could complicate things further by selecting a subset of participants scoring 42 or more in the scale Satisfaction with life (life.satis \\(\\geq\\) 42) and living alone (alone = 1). affective.dis[affective.dis$life.satis &gt;= 42 &amp; affective.dis$alone == 1, ] ## id treatment year sex depression life.satis alone ## 5 5 1 2020 male 16 68.70935 1 ## 8 8 1 2020 female 19 44.29188 1 ## 9 9 1 2020 male 17 50.12402 1 ## 10 10 1 2020 female 14 54.88683 1 ## 16 16 2 2020 female 7 52.87438 1 ## 20 20 2 2020 female 7 49.12089 1 ## 21 21 2 2020 male 16 61.50143 1 Logical operators x is less than y: x &lt; y x is less than or equal to y: x &lt;= y x is greater than y: x &gt; y x is greater than or equal to y: x &gt;= y x is equal to y: x == y Not equal to: != Not x: !x x AND y: x &amp; y x OR y: x | y "],["adding-new-data-to-a-data-frame.html", "3.2 Adding new data to a data frame", " 3.2 Adding new data to a data frame The first approach is very straightforward. We will add the name of the new variable (age) after the data frame (e.g., affective.dis) followed by the $ symbol. Then, we will assign the set of values of the newly-created variable to the data frame. age &lt;- sample(18:30, 36, replace = T) affective.dis$age &lt;- age head(affective.dis) ## id treatment year sex depression life.satis alone age ## 1 1 1 2020 male 22 53.50930 0 23 ## 2 2 1 2020 female 19 42.10586 0 19 ## 3 3 1 2020 male 14 22.73856 1 24 ## 4 4 1 2020 female 23 18.42421 0 21 ## 5 5 1 2020 male 16 68.70935 1 20 ## 6 6 1 2020 female 11 40.22030 1 21 With the second approach, we can merge variables (i.e., columns) and observations (i.e., rows) using the functions cbind() and rbind(). The function cbind() binds columns (i.e., variables), whereas rbind() binds rows (i.e., observations, cases, participants). y &lt;- sample(0:20, 36, replace = T) affective.dis &lt;- cbind(affective.dis, y) head(affective.dis) ## id treatment year sex depression life.satis alone age y ## 1 1 1 2020 male 22 53.50930 0 23 0 ## 2 2 1 2020 female 19 42.10586 0 19 4 ## 3 3 1 2020 male 14 22.73856 1 24 2 ## 4 4 1 2020 female 23 18.42421 0 21 11 ## 5 5 1 2020 male 16 68.70935 1 20 2 ## 6 6 1 2020 female 11 40.22030 1 21 0 To rearrange the order of the variables after adding new ones (e.g., placing age after sex), we can use indexing approaches to vectors. affective.dis &lt;- affective.dis[c(1:4, 8, 5:7, 9)] head(affective.dis) ## id treatment year sex age depression life.satis alone y ## 1 1 1 2020 male 23 22 53.50930 0 0 ## 2 2 1 2020 female 19 19 42.10586 0 4 ## 3 3 1 2020 male 24 14 22.73856 1 2 ## 4 4 1 2020 female 21 23 18.42421 0 11 ## 5 5 1 2020 male 20 16 68.70935 1 2 ## 6 6 1 2020 female 21 11 40.22030 1 0 "],["renaming-recoding-and-sorting-data.html", "3.3 Renaming, recoding, and sorting data", " 3.3 Renaming, recoding, and sorting data 3.3.1 Renaming To rename variables, we will use the indexing approach (i.e., square brackets) with the function names(). The assignment operator will assign the new names to the variables of our data set. For example, to rename y (the number of friends) and use a more appropriate name (i.e., friends), we will assign the new variable name to the 9th column of the data set (i.e., y). names(affective.dis) [9] &lt;- &#39;friends&#39; head(affective.dis) ## id treatment year sex age depression life.satis alone friends ## 1 1 1 2020 male 23 22 53.50930 0 0 ## 2 2 1 2020 female 19 19 42.10586 0 4 ## 3 3 1 2020 male 24 14 22.73856 1 2 ## 4 4 1 2020 female 21 23 18.42421 0 11 ## 5 5 1 2020 male 20 16 68.70935 1 2 ## 6 6 1 2020 female 21 11 40.22030 1 0 3.3.2 Recoding We can recode variables in different columns (i.e., creating a new variable with the new codes and keep the old variable) or in the same column (i.e., overwriting the existing variable). In the following example, we are creating a new variable called satis. This new ordinal variable displays three ordered bands of the scale Satisfaction with life (e.g., Low, Medium, High) with cut-off points set at 26 and 47. We have to recode the values of life.satis into the new variable satis using conditional subsetting with logical operators. affective.dis$satis [affective.dis$life.satis &lt; 26] &lt;- &#39;Low&#39; affective.dis$satis [affective.dis$life.satis &gt;= 26 &amp; affective.dis$life.satis &lt; 47] &lt;- &#39;Medium&#39; affective.dis$satis [affective.dis$life.satis &gt;= 47] &lt;- &#39;High&#39; affective.dis &lt;- affective.dis[c(1:7, 10, 9, 8)] head(affective.dis) ## id treatment year sex age depression life.satis satis friends alone ## 1 1 1 2020 male 23 22 53.50930 High 0 0 ## 2 2 1 2020 female 19 19 42.10586 Medium 4 0 ## 3 3 1 2020 male 24 14 22.73856 Low 2 1 ## 4 4 1 2020 female 21 23 18.42421 Low 11 0 ## 5 5 1 2020 male 20 16 68.70935 High 2 1 ## 6 6 1 2020 female 21 11 40.22030 Medium 0 1 3.3.3 Sorting If we are interested in sorting our data set for a visual inspection we will use the function order(). The argument decreasing set as FALSE shows the lowest values first, whereas the argument decreasing set as TRUE displays the highest values first. affective.dis &lt;- affective.dis[order(affective.dis$depression, decreasing = T), ] head(affective.dis) ## id treatment year sex age depression life.satis satis friends alone ## 7 7 1 2020 male 30 29 32.00596 Medium 9 0 ## 4 4 1 2020 female 21 23 18.42421 Low 11 0 ## 1 1 1 2020 male 23 22 53.50930 High 0 0 ## 12 12 1 2020 female 26 22 48.86415 High 13 0 ## 11 11 1 2020 male 27 20 40.08320 Medium 18 1 ## 2 2 1 2020 female 19 19 42.10586 Medium 4 0 We might be also interested in inspecting depression scores as a function of two or more variables. For instance, to inspect depression by age groups, we will include two arguments in the function order(). First, we will order the observations by age. Then, within every year, we will order our observations by depression. affective.dis &lt;- affective.dis[order(affective.dis$age, affective.dis$depression), ] head(affective.dis) ## id treatment year sex age depression life.satis satis friends alone ## 36 36 3 2020 female 18 4 41.11086 Medium 4 0 ## 34 34 3 2020 female 18 9 62.02641 High 10 0 ## 29 29 3 2020 male 19 9 58.75117 High 2 0 ## 17 17 2 2020 male 19 13 38.76411 Medium 10 1 ## 10 10 1 2020 female 19 14 54.88683 High 18 1 ## 2 2 1 2020 female 19 19 42.10586 Medium 4 0 When we use the function order(), we change the original order of the rows of our data set. It is always convenient to restore the original order by using the variable id (1:n). Remember to leave a blank space after the comma located within the square brackets (i.e., [rows , columns]) to sort the rows by id. affective.dis &lt;- affective.dis[order(affective.dis$id), ] "],["the-tidyverse.html", "3.4 The tidyverse", " 3.4 The tidyverse The tidyverse is a collection of packages developed by a team of programmers led by Hadley Wickham that share the same philosophy and grammar (Wickham &amp; Grolemund, 2017). The main goal of the tidyverse is to favor an easy and comprehensive approach to data science by solving the challenges that programmers usually face when coding in R (Wickham et al., 2019). Some of the core packages of the tidyverse (e.g., readr, tidyr, dplyr, ggplot2) provide clear insights on their functionality as they will provide solutions to import, tidy, manipulate, and visualize data. 3.4.1 The pipe (%&gt;%) The “pipe” operator is used to connect multiple verb actions into a pipeline. In sum, we can chain or pipe functions to perform a sequence of operations (e.g., data wrangling). The package magrittr offers a set of operators which make your R code more readable by: structuring sequences of data operations left-to-right (as opposed to from the inside and out). avoiding nested function calls. minimizing the need for local variables and function definitions. making it easy to add steps anywhere in the sequence of operations. The operators pipe their left-hand side values forward into expressions that appear on the right-hand side; i.e., one can replace f(x) with x %&gt;% f(), where %&gt;% is the (main) pipe-operator. The flow of the R code will be as follows: data %&gt;% function 1 %&gt;% function 2 %&gt;% function 3 ... Keyboard shortcut for the pipe operator Windows: Press Ctrl + Shift + M MacOS: Press Cmd + Shift + M 3.4.2 The package dplyr The package dplyr was developed by Hadley Wickham and colleagues. It is an optimized and distilled version of an old package named plyr. The package dplyr does not provide any “new” functionality to R per se. Everything dplyr does could already be done with base R, but it greatly simplifies existing functionality in R (Wickham et al., 2022). One important contribution of the package dplyr is that it provides a “grammar” (in particular, verbs) for data manipulation and for operating on data frames. Other people can understand what we are doing to a data frame with this \"grammar.\" This new approach is very useful because it provides an abstraction for data manipulation that it did not exist before. Another useful contribution of the package dplyr is that their functions are very fast, because many key operations are coded in C++. dplyr verbs select: It returns a subset of the columns of a data frame, using a flexible notation filter: It extracts a subset of rows from a data frame based on logical conditions relocate: It reorders the columns of a data frame arrange: It reorders the rows of a data frame rename: It renames variables in a data frame mutate: It adds new variables/columns or transform existing variables summarize: It generates a summary statistics of different variables in the data frame 3.4.3 The package tidyr The package tidyr was also developed by Hadley Wickham and colleagues. The package tidyr was created to tidy data; i.e., every vector column represents a variable, every row includes information of one participant/observation, and cells include a single value. The package tidyr is widely used to transform data frames from wide to long format and viceversa. This process involving the reshaping of data relies on pivoting the data set to collapse several columns into fewer columns (pivot_longer()) to generate a long-format data set, or to expand a few columns into more columns (pivot_wider()) to create a wide-format data set. Let's select the first four observations of the simulated data set affective.dis, a wide-format data set in which participant's responses were entered in a single row and multiple columns. We will use the function slice_sample() from the package dplyr to subset n random rows, rather than using the function filter() that requires setting conditions to keep some rows while dropping others (e.g., filter(sex == 'female')). affective.wide1 &lt;- affective.dis %&gt;% slice_sample(n = 5) affective.wide1 ## id treatment year sex age depression life.satis satis friends alone ## 1 7 1 2020 male 30 29 32.00596 Medium 9 0 ## 2 22 2 2020 female 30 13 44.87210 Medium 3 0 ## 3 23 2 2020 male 26 11 49.37174 High 19 0 ## 4 29 3 2020 male 19 9 58.75117 High 2 0 ## 5 12 1 2020 female 26 22 48.86415 High 13 0 XXXXXX TODO!! XXXXXX "],["data-wrangling-with-the-tidyverse.html", "3.5 Data wrangling with the tidyverse", " 3.5 Data wrangling with the tidyverse First of all, we need to load the tidyverse that includes a suite of packages such as dplyr, tidyr, or ggplot2. If we are not loading the tidyverse or any of its packages, we will need to upload the package magrittr to use the pipe operator. library(tidyverse) We are going to use the simulated data set affective.dis to perform several operations that will end with the estimation of two descriptive statistics (M and SD) of the variables depression and life.satis for the female participants only and split by the three experimental conditions of the variable treatment. First, the data set affective.dis will be piped to the first operation: to select the variables id, treatment, sex, depression, and life.satis. Second, the resulting data set will be piped to the second operation: to filter a subset of rows based on the logical condition sex == 'female' (i.e., selecting only the rows/participants that are females). Third, we will split the following operations by the three experimental groups of the variable treatment. Last, we will compute the means and standard deviations of the variables depression and life.satis. affective.dis %&gt;% select(id, treatment, sex, depression, life.satis) %&gt;% filter(sex == &#39;female&#39;) %&gt;% group_by(treatment) %&gt;% summarise(mean_depression = mean(depression), sd_depression = sd(depression), mean_life.satis = mean(life.satis), sd_life.satis = sd(life.satis)) ## # A tibble: 3 × 5 ## treatment mean_depression sd_depression mean_life.satis sd_life.satis ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 18 4.65 41.5 12.5 ## 2 2 9.83 3.06 41.7 9.47 ## 3 3 6.5 3.51 51.2 9.90 Sometimes, we are interested in using pipes to make sequential operations in our data sets, but the final goal is not to produce an output, but to modify a data set and to keep those changes. In these cases, we will use the assignment operator to save these changes into the same R object or into a different one. For example, in the following R code, (1) we will drop the variable year because it is a constant, (2) we will transform the variable treatment into a factor—assigning labels to its three experimental conditions, (3) we will move the variables sex and age to the second and third column respectively and the variable satis after the variable life.satis, (4) instead of overwriting the new data set on the former R object affective.dis, we will assign the new data set to the R object affective.dis2, (5) we will rename the variable friends to convey that the information stored in that vector column is about the number of friends (n.friends) that each participant has, and (6) we will reorder the rows of the new data set from the first (id = 1) to the last (id = 36) participant. affective.dis2 &lt;- affective.dis %&gt;% select(-year) %&gt;% mutate(treatment = factor(treatment, labels = c(&#39;Exercise&#39;, &#39;Mindfulness&#39;, &#39;CBT&#39;))) %&gt;% relocate(sex, age, .after = id) %&gt;% relocate(satis, .after = life.satis) %&gt;% rename(n.friends = friends) %&gt;% arrange(id) affective.dis2 ## id sex age treatment depression life.satis satis n.friends alone ## 1 1 male 23 Exercise 22 53.50930 High 0 0 ## 2 2 female 19 Exercise 19 42.10586 Medium 4 0 ## 3 3 male 24 Exercise 14 22.73856 Low 2 1 ## 4 4 female 21 Exercise 23 18.42421 Low 11 0 ## 5 5 male 20 Exercise 16 68.70935 High 2 1 ## 6 6 female 21 Exercise 11 40.22030 Medium 0 1 ## 7 7 male 30 Exercise 29 32.00596 Medium 9 0 ## 8 8 female 25 Exercise 19 44.29188 Medium 11 1 ## 9 9 male 30 Exercise 17 50.12402 High 19 1 ## 10 10 female 19 Exercise 14 54.88683 High 18 1 ## 11 11 male 27 Exercise 20 40.08320 Medium 18 1 ## 12 12 female 26 Exercise 22 48.86415 High 13 0 ## 13 13 male 28 Mindfulness 11 31.99603 Medium 15 0 ## 14 14 female 26 Mindfulness 8 29.28087 Medium 1 0 ## 15 15 male 29 Mindfulness 5 64.95737 High 10 0 ## 16 16 female 22 Mindfulness 7 52.87438 High 3 1 ## 17 17 male 19 Mindfulness 13 38.76411 Medium 10 1 ## 18 18 female 29 Mindfulness 10 31.53965 Medium 5 0 ## 19 19 male 30 Mindfulness 9 39.57584 Medium 19 0 ## 20 20 female 30 Mindfulness 7 49.12089 High 9 1 ## 21 21 male 25 Mindfulness 16 61.50143 High 12 1 ## 22 22 female 30 Mindfulness 13 44.87210 Medium 3 0 ## 23 23 male 26 Mindfulness 11 49.37174 High 19 0 ## 24 24 female 23 Mindfulness 14 42.72718 Medium 19 0 ## 25 25 male 24 CBT 8 40.50052 Medium 16 0 ## 26 26 female 24 CBT 12 55.23754 High 2 0 ## 27 27 male 30 CBT 7 50.43363 High 7 0 ## 28 28 female 29 CBT 4 37.22818 Medium 12 0 ## 29 29 male 19 CBT 9 58.75117 High 2 0 ## 30 30 female 25 CBT 7 52.71514 High 2 0 ## 31 31 male 24 CBT 11 37.55278 Medium 15 0 ## 32 32 female 29 CBT 3 58.79335 High 12 0 ## 33 33 male 30 CBT 5 19.83120 Low 7 0 ## 34 34 female 18 CBT 9 62.02641 High 10 0 ## 35 35 male 21 CBT 14 19.23629 Low 16 0 ## 36 36 female 18 CBT 4 41.11086 Medium 4 0 "],["references-2.html", "3.6 References", " 3.6 References Gandrud, C. (2015). Reproducible Research with R and RStudio (2nd ed.). Chapman and Hall/CRC. Lander, J. P. (2017). R for everyone (2nd ed.). Addison-Wesley. R Core Team (2023). R: A language and environment for statistical computing. R Foundation for Statistical Computing. Venables, W. N., Smith, D. M., &amp; the R Development Core Team. (2009). An Introduction to R. Network Theory Limited. Wickham, H., &amp; Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data. O'Reilly. Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., Takahashi, K., Vaughan, D., Wilke, C., Woo, K., &amp; Yutani, H. (2019). Welcome to the Tidyverse. Journal of Open Source Software, 4(43), 1686. Retrieved at https://joss.theoj.org/papers/10.21105/joss.01686 Wickham, H., François, R., Henry, L., &amp; Müller, K. (2022). dplyr: A grammar of data manipulation. Retrieved at https://dplyr.tidyverse.org Xie, Y. (2015). Dynamic documents with R and knitr (2nd ed.). Chapman and Hall/CRC. Xie, Y., Allaire, J. J., &amp; Grolemund, G. (2019). R Markdown: The definite guide. Chapman and Hall/CRC. "],["descriptive-statistics-and-data-visualization.html", "Chapter 4 Descriptive Statistics and Data Visualization", " Chapter 4 Descriptive Statistics and Data Visualization LEARNING OUTCOMES Identify the capabilities of R and RStudio's environment and appraise their functionality. Distinguish between R functions, objects, and diverse data wrangling approaches. Apply basic programming skills to import and organize the data using classic data wrangling approaches and the tidyverse grammar. Evaluate the R code and appraise the outputs to demonstrate a satisfactory level of basic programming skills in R. XXXXXXX TODO!! XXXXXXX "],["modeling-in-r.html", "Chapter 5 Modeling in R", " Chapter 5 Modeling in R LEARNING OUTCOMES Identify the capabilities of R and RStudio's environment and appraise their functionality. Distinguish between R functions, objects, and diverse data wrangling approaches. Apply basic programming skills to import and organize the data using classic data wrangling approaches and the tidyverse grammar. Evaluate the R code and appraise the outputs to demonstrate a satisfactory level of basic programming skills in R. XXXXX TODO!! XXXXXX The purpose of a statistical model is to help understand what variables might best predict a phenomenon of interest, which ones have more or less influence, define a predictive equation with coefficients for each of the variables, and then apply that equation to predict values using the same input variables for other areas. This process requires samples with observations of the explanatory (or independent) and response (or dependent) variables in question. "],["some-common-statistical-models.html", "5.1 Some common statistical models", " 5.1 Some common statistical models There are many types of statistical models. Variables may be nominal (categorical) or interval/ratio data. You may be interested in predicting a continuous interval/ratio variable from other continuous variables, or predicting the probability of an occurrence (e.g. of a species), or maybe the count of something (also maybe a species). You may be needing to classify your phenomena based on continuous variables. Here are some examples: lm(y ~ x) linear regression model with one explanatory variable lm(y ~ x1 + x2 + x3) multiple regression, a linear model with multiple explanatory variables glm(y ~ x, family = poisson) generalized linear model, poisson distribution; see ?family to see those supported, including binomial, gaussian, poisson, etc. glm(y ~ x + y, family = binomial) glm for logistic regression aov(y ~ x) analysis of variance (same as lm() except in the summary) gam(y ~ x) generalized additive models tree(y ~ x) or rpart(y ~ x) regression/classification trees here!! "],["linear-models-i.html", "Chapter 6 Linear Models I", " Chapter 6 Linear Models I LEARNING OUTCOMES Identify the capabilities of R and RStudio's environment and appraise their functionality. Distinguish between R functions, objects, and diverse data wrangling approaches. Apply basic programming skills to import and organize the data using classic data wrangling approaches and the tidyverse grammar. Evaluate the R code and appraise the outputs to demonstrate a satisfactory level of basic programming skills in R. XXXXX TODO!!! XXXX XXXXXXX The purpose of a statistical model is to help understand what variables might best predict a phenomenon of interest, which ones have more or less influence, define a predictive equation with coefficients for each of the variables, and then apply that equation to predict values using the same input variables for other areas. This process requires samples with observations of the explanatory (or independent) and response (or dependent) variables in question. t-tests, one way anova "],["linear-models-ii.html", "Chapter 7 Linear Models II", " Chapter 7 Linear Models II LEARNING OUTCOMES Identify the capabilities of R and RStudio's environment and appraise their functionality. Distinguish between R functions, objects, and diverse data wrangling approaches. Apply basic programming skills to import and organize the data using classic data wrangling approaches and the tidyverse grammar. Evaluate the R code and appraise the outputs to demonstrate a satisfactory level of basic programming skills in R. The purpose of a statistical model is to help understand what variables might best predict a phenomenon of interest, which ones have more or less influence, define a predictive equation with coefficients for each of the variables, and then apply that equation to predict values using the same input variables for other areas. This process requires samples with observations of the explanatory (or independent) and response (or dependent) variables in question. Interactions + factorial anova + ANCOVA "],["linear-models-iii.html", "Chapter 8 Linear Models III", " Chapter 8 Linear Models III LEARNING OUTCOMES Identify the capabilities of R and RStudio's environment and appraise their functionality. Distinguish between R functions, objects, and diverse data wrangling approaches. Apply basic programming skills to import and organize the data using classic data wrangling approaches and the tidyverse grammar. Evaluate the R code and appraise the outputs to demonstrate a satisfactory level of basic programming skills in R. The purpose of a statistical model is to help understand what variables might best predict a phenomenon of interest, which ones have more or less influence, define a predictive equation with coefficients for each of the variables, and then apply that equation to predict values using the same input variables for other areas. This process requires samples with observations of the explanatory (or independent) and response (or dependent) variables in question. linear regression with factors and quant variables + robust regression "],["generalized-linear-models.html", "Chapter 9 Generalized Linear Models", " Chapter 9 Generalized Linear Models LEARNING OUTCOMES Identify the capabilities of R and RStudio's environment and appraise their functionality. Distinguish between R functions, objects, and diverse data wrangling approaches. Apply basic programming skills to import and organize the data using classic data wrangling approaches and the tidyverse grammar. Evaluate the R code and appraise the outputs to demonstrate a satisfactory level of basic programming skills in R. The purpose of a statistical model is to help understand what variables might best predict a phenomenon of interest, which ones have more or less influence, define a predictive equation with coefficients for each of the variables, and then apply that equation to predict values using the same input variables for other areas. This process requires samples with observations of the explanatory (or independent) and response (or dependent) variables in question. logistic and count data (poisson) "],["categorical-data-analysis.html", "Chapter 10 Categorical Data Analysis", " Chapter 10 Categorical Data Analysis LEARNING OUTCOMES Distinguish between the different types of categorical data and their underlying sampling distributions. Identify the statistical tests proposed for categorical data analysis of independent and matched data. Apply R's functions to estimate the parameters of categorical models and evaluate the R code and appraise the outputs. Produce contingency tables and data visualizations to summarize categorical models. A categorical variable assigns numerical values to a set of discrete categories. The measurement level of categorical data ranges from assigning two values to two distinct categories (binomial data or dichotomous variables), to assign values to more than two unordered categories (multinomial data, nominal data, or polytomous variables), to assign values to more than two ordered categories (ordinal data or ordinal variables). In some cases, we do not have frequency data from independent units or individuals, but a number of events that are not independent. TODO! provide example of counts. "],["categorical-data.html", "10.1 Categorical data", " 10.1 Categorical data 10.1.1 Binomial and multinomial data Binomial and multinomial data (also called binary and nominal data) include the property of identity. For each case and observation that we will measure, we will assign one value that will identify that case or observation with only one category (e.g., Helping a person in distress versus Ignoring a person in distress). The set of categories has to be exhaustive, including all the possible categories. For example, if we decide to classify bystanders as individuals helping or ignoring someone in distress, we might find some instrumental behaviors that are difficult to classify as pro-social and anti-social. Consequently, we must include an additional category that could capture other behaviors (Other behaviors). Similarly, if we have a large number of categories (e.g., political parties in a general election), we could include the main ones (e.g., Conservative, Labour, and Liberal Democrats) and collapse the remaining categories under a general category (e.g., Other political parties). Because the main property of this measurement level is to belong to one single category, the numeric value that we assign to each category is arbitrary. For instance, we could assign a 0 to the category Fail of the binary variable Grades and a 1 to the category Pass as it is easier to recall the labels of this variable and to interpret the outputs of the model. However, assigning a 10 to Fail and a 3 to Pass is also possible because the values 10 and 3 are different. 10.1.2 Ordinal data Ordinal data include the property of identity and the property of order. When we are measuring observations or cases that could be ordered, we will assign values to the ordered categories that will imply a higher or lower rank. The most basic ordinal variable includes three categories (e.g., Against, Undecided, and In favor). Some ordinal variables are bipolar (i.e., two poles or extremes; e.g., Bad, Neutral, and Good), whereas others are unipolar (i.e., one pole or extreme; e.g., No income, Annual gross salary lower than £25,000, Annual gross salary between £25,000 and £50,000, and Annual gross salary higher than £50,000). In psychology, Likert scales are very popular ordinal variables. We assume a monotonic increase in the ordered categories and provide meaningful anchoring points with their associated labels (e.g., Strongly disagree, Disgree, Slightly disagree, Neither disagree/agree, Strongly agree, Agree, and Slightly agree). We usually assign a 0 or 1 to the first category and increase by one point the values assigned to the rest of the ordered categories. Psychologists tend to assume that Likert scales with 5 or more anchoring points could be treated as quasi-interval. TODO! "],["dealing-with-categorical-variables-in-r.html", "10.2 Dealing with categorical variables in R", " 10.2 Dealing with categorical variables in R Categorical data are often called factors when we assign values implying identity or order properties to character variables (e.g., Helping behavior, Grades). In R, the function factor() allows us to transform a variable into an unorder (binomial and multinomial data) or order (ordinal data) variable. The arguments levels and labels are used to set the values and labels for each category respectively. Let's simulate the data set called help.data that includes observations of 18 male and female bystanders who decided to help or not a person in distress and who were asked the extent to which they donate money to charities. We can create the vectors first and then bind them to create a data frame with the function data.frame. We can explore the structure of the R object with the function str() to find out that the variable sex is a character vector, whereas help.behavior and donation are numeric vectors. id &lt;- 1:18 sex &lt;- c(&#39;male&#39;, &#39;female&#39;, &#39;male&#39;, &#39;male&#39;, &#39;male&#39;, &#39;male&#39;, &#39;female&#39;, &#39;female&#39;, &#39;male&#39;, &#39;female&#39;, &#39;female&#39;, &#39;female&#39;, &#39;female&#39;, &#39;male&#39;, &#39;male&#39;, &#39;female&#39;, &#39;female&#39;, &#39;male&#39;) help.behavior &lt;- c(1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1) donation &lt;- c(0, 1, 1, 0, 0, 2, 0, 3, 1, 2, 2, 0, 1, 0, 3, 1, 2, 3) coop &lt;- c(2, 1, 2, 2, 1, 3, 1, 2, 2, 3, 1, 2, 2, 1, 3, 3, 2, 3) help.data &lt;- data.frame(id, sex, help.behavior, donation, coop) help.data ## id sex help.behavior donation coop ## 1 1 male 1 0 2 ## 2 2 female 1 1 1 ## 3 3 male 0 1 2 ## 4 4 male 0 0 2 ## 5 5 male 0 0 1 ## 6 6 male 0 2 3 ## 7 7 female 0 0 1 ## 8 8 female 1 3 2 ## 9 9 male 0 1 2 ## 10 10 female 1 2 3 ## 11 11 female 1 2 1 ## 12 12 female 0 0 2 ## 13 13 female 0 1 2 ## 14 14 male 0 0 1 ## 15 15 male 1 3 3 ## 16 16 female 1 1 3 ## 17 17 female 1 2 2 ## 18 18 male 1 3 3 str(help.data) ## &#39;data.frame&#39;: 18 obs. of 5 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ sex : chr &quot;male&quot; &quot;female&quot; &quot;male&quot; &quot;male&quot; ... ## $ help.behavior: num 1 1 0 0 0 0 0 1 0 1 ... ## $ donation : num 0 1 1 0 0 2 0 3 1 2 ... ## $ coop : num 2 1 2 2 1 3 1 2 2 3 ... Using the function mutate() from the package dplyr we can transform our three character and numeric variables into factors. For doing so, we will use the function factor() to select the variable that we want to transform into a factor as the first argument, followed by three more arguments: levels (i.e., to set the levels of the factors), labels (i.e., to name the levels), and ordered (i.e., to specify if we deal with binomial/multinomial or ordinal data). library(dplyr) help.data &lt;- help.data %&gt;% mutate( sex = factor(sex, levels = c(&#39;male&#39;, &#39;female&#39;), labels = c(&#39;Male&#39;, &#39;Female&#39;), ordered = F), help.behavior = factor(help.behavior, levels = c(0, 1), labels = c(&#39;No&#39;, &#39;Yes&#39;), ordered = F), donation = factor(donation, levels = c(0, 1, 2, 3), labels = c(&#39;Never&#39;, &#39;Sometimes&#39;, &#39;Often&#39;, &#39;Always&#39;), ordered = T), coop = factor(coop, levels = c(1, 2, 3), labels = c(&#39;Untrue of me&#39;, &#39;Neutral&#39;, &#39;True of me&#39;), ordered = T)) str(help.data) ## &#39;data.frame&#39;: 18 obs. of 5 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ sex : Factor w/ 2 levels &quot;Male&quot;,&quot;Female&quot;: 1 2 1 1 1 1 2 2 1 2 ... ## $ help.behavior: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 1 1 1 1 1 2 1 2 ... ## $ donation : Ord.factor w/ 4 levels &quot;Never&quot;&lt;&quot;Sometimes&quot;&lt;..: 1 2 2 1 1 3 1 4 2 3 ... ## $ coop : Ord.factor w/ 3 levels &quot;Untrue of me&quot;&lt;..: 2 1 2 2 1 3 1 2 2 3 ... "],["research-questions-with-categorical-data.html", "10.3 Research questions with categorical data", " 10.3 Research questions with categorical data The way of summarizing categorical data is by estimating frequencies, proportions, and percentages for binary and nominal data, and the median and mean ranks for ordinal data. Let's take a look at Table 10.1. The table includes ten popular social media sites. The categorical variable Table 10.1: Social Media Sites Item Social media site 1 Facebook 2 Twitter 3 WhatsApp 4 Instagram 5 TikTok 6 Snapchat 7 YouTube 8 Twitch 9 WeChat 10 Telegram XXXXXXX TODO &gt;&gt; binomial + chi-square + mcnemar + log-odds + vcd "],["references-3.html", "10.4 References", " 10.4 References Arslan, R. C. (2019). How to automatically document data with the codebook package to facilitate data reuse. Advances in Methods and Practices in Psychological Science, 2(2), 169—187. European Commission. (2021). Open Science [Fact sheet]. European Union. Gewin, V. (2016). Data sharing: An open mind on open data. Nature, 529, 117—119. Hunt, L. T. (2019). The life-changing magic of sharing your data. Nature Human Behavior, 3, 312—315. John, L. K., Loewenstein, G., &amp; Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23(5), 524—532. Klein O., Hardwicke T. E., Aust F., Breuer J., Danielsson H., Mohr A. H., Ijzerman H., Nilsonne G., Vanpaemel W., Frank M. C. (2018). A practical guide for transparency in psychological science. Collabra: Psychology, 4(1):20. Kovacs, M., Hoekstra, R., &amp; Aczel, B. (2021). The role of human fallibility in psychological research: A survey of mistakes in data management. Advances in Methods and Practices in Psychological Science, 4(4), 1—13. Miedema, F. (2022). Open Science: The very idea. Springer. Nature--Scientific Data (2023). Frequently Asked Questions. Retrieved from Nature--Scientific Data website: https://www.nature.com/sdata/faq Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), 1—8. Spellman, B., Gilbert, E. A., &amp; Corker, K. S. (2017, September 20). Open Science: What, Why, and How. https://psyarxiv.com/ak6jr/ UK Reproducibility Network. (2021, September). Terms of Reference. Version 3.3. United Nations Educational, Scientific, and Cultural Organization. (2021, November). UNESCO Recommendation on Open Science (Programme Document SC-PCB-SPP/2021/OS/UROS). Retrieved from the United Nations Educational, Scientific, and Cultural Organization website: https://unesdoc.unesco.org/ark:/48223/pf0000379949 Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J. W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., Gonzalez-Beltran, A., … Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3:160018. "],["linear-mixed-models.html", "Chapter 11 Linear Mixed Models", " Chapter 11 Linear Mixed Models LEARNING OUTCOMES Identify the capabilities of R and RStudio's environment and appraise their functionality. Distinguish between R functions, objects, and diverse data wrangling approaches. Apply basic programming skills to import and organize the data using classic data wrangling approaches and the tidyverse grammar. Evaluate the R code and appraise the outputs to demonstrate a satisfactory level of basic programming skills in R. The purpose of a statistical model is to help understand what variables might best predict a phenomenon of interest, which ones have more or less influence, define a predictive equation with coefficients for each of the variables, and then apply that equation to predict values using the same input variables for other areas. This process requires samples with observations of the explanatory (or independent) and response (or dependent) variables in question. t-tests, one way anovas "],["scale-development.html", "Chapter 12 Scale Development", " Chapter 12 Scale Development LEARNING OUTCOMES Define measurement and give several examples of measurement in psychology. Explain what is a psychological construct and give several examples. Distinguish the four levels of measurement, give examples of each level, and explain why this distinction is important. Identify the different classification systems of psychological tests and critically evaluate their implications for scale construction. Name the stages and evaluate the iterative process of scale development. Psychometrics is the science of psychological assessment. It is a subfield of methodology that studies the process of measuring psychological and social phenomena using formal mathematical models. What do psychologists measure? Do psychologists measure physical objects or hypothetical entities? Although many classic psychological studies have measured overt behaviors (e.g., reaction time, occurrence of a behavioral repertoire), the measurement of behavior has decreased exponentially after the cognitive revolution. Nowadays, psychologists usually ask respondents to complete questionnaires, inventories, and scales by clicking on a computer screen or by ticking a box in paper-and-pencil psychological tests. Psychologists tend to measure unobservable variables that are operationalized in various ways. Thus, a psychological construct (i.e., a latent variable) is a theoretical psychological entity that cannot be measured directly. To measure latent variables, we need to develop instruments that will include items or tasks devised to measure the underlying psychological construct. Consequently, the observed variables (i.e., measures) are assumed to be probabilistic functions of underlying unobservable variables (i.e., latent variables). Latent and observed variables Guided by a theory, specify what do you want to measure. For example, if we are interested in measuring the psychological construct extraversion, we need to define theoretically the limits of the construct before deciding how to measure it using observed variables Make decisions on the purpose of the test and on how you will assign numbers (i.e., scaling) to the observed variables. Should we construct categorical, ordinal, or interval scales? Should we use a knowledge-based or a person-based questionnaire? Should we use norm-referenced or criterion-referenced tests? Generate a psychometric model of the data to estimate the latent variables. We can use data-reduction techniques (e.g., exploratory factor analysis) guided by theory. We also need to evaluate how reliable is our psychometric model Model the structure of the latent variables to check whether our data fit the model (i.e., the theory) or not. We can also compare between different models to assess which one provides a better fit How can we measure psychological constructs such as intelligence, neuroticism, psychological well-being, students' satisfaction, academic performance, attitudes towards refugees and migrants, or depression? We must transform observations into data and to apply formal mathematical models to assign numbers to subjects' attributes relying on their responses or on the stimuli observed. In sum, we need to assign numbers to observed psychological phenomena and to unobserved constructs, while assessing the fit of theoretical models to data. "],["reflective-and-formative-indicators.html", "12.1 Reflective and formative indicators", " 12.1 Reflective and formative indicators Psychologists mainly work with unobservable entities (e.g., depression, impulsiveness, intelligence), not with the items or scales (i.e., measures). The items of a given test are the indirect way to measure the underlying psychological construct (i.e., a latent variable). The domain covered by any psychological construct is sampled through items. Consequently, a scale (e.g., 9 items measuring job burnout) intends to reflect the underlying psychological construct (i.e., job burnout). Latent variables are regarded as causes of item scores. The magnitude (i.e., strength) of the latent variable is assumed to cause an item to take on a certain value. Sometimes, some subdomains emerge (i.e., factors). These subdomains or factors are identified using iterative conceptual and empirical analyses (e.g., exploratory factor analysis). In sum, observed measures (e.g., items) reflect the latent variable (Figure 12.1). Figure 12.1: Reflective and formative indicator models. "],["classification-of-scales-of-measurement.html", "12.2 Classification of scales of measurement", " 12.2 Classification of scales of measurement Psychologists have taken an a priori approach to scaling, avoiding the problems related to representation, uniqueness, invariance, and meaningfulness (Krantz et al., 1970). Although the properties of interval scales are often violated, we tend to assume that the values assigned to the responses given to psychological tests, questionnaires, or inventories form an interval scale. Let's review the popular classification of scales of measurement defined by Stevens (1946) (Figure 12.2). Figure 12.2: Classification of scales. 12.2.1 Nominal scale It consists of qualitative and unordered categories (e.g., political parties of the US Presidential Election-2016). The property of this type of scale is identity (i.e., X does/doesn't belong to the category Y). Nominal scales have two distinctive features: Categories should be mutually exclusive. Categories should be exhaustive. 12.2.2 Ordinal scale It orders people, subjects, events, or characteristics along some continuum (e.g., intensity of verbal aggression towards football referees). The property of this type of scale is order (i.e., X is more/less than Y). Ordinal scales have two distinctive features: Numbers place objects in order. No information is given about the differences between points of the scale. 12.2.3 Interval scale It is a scale on which equal intervals between objects represent equal differences (e.g., temperature). The property of this type of scale is difference (i.e., the difference between X and Y is the same as the difference between A and B). Interval scales have three distinctive features: Differences are meaningful. No information is given about ratios. The value zero is arbitrary. 12.2.4 Ratio scale It is a scale that includes a true zero that corresponds to the absence of the characteristic being measured (e.g., reaction time). The property of this type of scale is ratio (i.e., X is twice as long as Y). Interval scales have one distinctive feature: Ratios are meaningful. "],["test-theory.html", "12.3 Test theory", " 12.3 Test theory Test theory is grounded on statistics and the theory of probability. It assumes that the values assigned to the observations (i.e., subjects' responses) constitute a scale. The final goal of test theory is to locate the subjects or the stimuli along a continuum of a non-observable attribute (i.e., a latent trait). 12.3.1 What is a test? A test is a device or procedure in which a sample of a respondent's behavior in a specified domain is obtained, evaluated, and scored using a standardized process (Standards for Educational and Psychological Tests, 2014). On the one hand, psychological tests include instruments measuring the correctness or quality of the responses (e.g., measures of ability, aptitude). On the other hand, tests are also used to measure attitudes, preferences, and predispositions (e.g., measures of personality, mental health, cognitive functioning, attitudes). CAUTION! There is no unique and universally accepted measure for any given construct. It is required to examine the theoretical framework and its specifications All tests are based on limited samples of behavior extracted from the defined population of the domain There is always a measurement error (sampling error) Absence of scales with origin and unit of measurement (i.e., indetermination of the measurement scale) Latent variables cannot be defined in isolation, but by establishing its nomological network with other constructs and observed variables 12.3.2 Classification of psychological tests Type of domain: Tests are usually classified as cognitive (e.g., intelligence, ability, academic performance) and non-cognitive tests (e.g., personality, attitudes, motivations, mood). Other authors refer to these two groups of tests as ability (measures of maximum performance) versus personality (measures of typical performance) tests. Consequences for the subject: Tests can be placed into a continuum ranging from high- to low-stakes. If the test's scores are used to determine an important outcome (e.g., to get a job, A levels, clinical screening), then it is a high-stakes test. Response format/task: The most common response formats range from a selected-response (i.e., the respondent selects the correct, best, or most appropriate response from a list of possible answers to questions or stimuli) to a constructed-response item format (i.e., the respondent produces a written response to a question or stimulus). Administration: Tests can be classified according to the administration mode. For example, a psychological test can be administered individually versus collectively, paper-and-pencil versus on-line tests, or adaptive versus non-adaptive tests. Response-time constraints: Some psychological tests are classified as speed tests when the items are rather easy but include a time limit. On the other hand, tests classified as power tests are designed to measure the ability of the respondent, not the speed when performing these tests. Statistical model: Tests can be classified in relation to the statistical model in which the test scores are based. Classical Test Theory (CTT) is based on the general linear model and test total scores. Item Response Theory (IRT) is based on a nonlinear model estimating the probability to answer to a given item as a function of the different levels of the underlying trait or ability. Interpretation of the scores: Some tests focus on the degree of coverage of a certain domain or a predetermined criterion (i.e., criterion-referenced tests). Other tests, however, focus on the relative position of the respondent in relation to a reference group (i.e., norm-referenced tests). 12.3.3 Measurement error In applied statistics and psychometrics we model data. Models are representations of reality and they do not often fit well into our data. Even in disciplines such as physics, some randomness is expected. Measurement error is pervasive and we need to include it when we model our data. Thus, our data result from adding to the model some measurement error (it is often called residual). \\[\\begin{aligned} Data = Model \\ + \\ Residual\\\\ \\end{aligned}\\] In turn, the measurement error will result from subtracting our model to our data. Put differently, the model is what we know (e.g., the theory X that we used to explain job burnout) and the residual or error is what we still do not know (e.g., what remains unexplained by theory X in relation to the phenomenon of job burnout). \\[\\begin{aligned} Residual = Data \\ - \\ Model\\\\ \\end{aligned}\\] In psychometrics, we assume that any observed score (\\(X_{i}\\)) is the result of a true score (\\(T_{i}\\)) plus its measurement error (\\(E_{i}\\)). Thus, the true score is the quantity that one subject has for a given trait, ability, or attribute. The observed score and the measurement error are random variables, whereas the true score is assumed to be a constant (the score obtained after infinite number of observations on the same subject). \\[\\begin{aligned} X_{i} = T_{i} \\ + \\ E_{i}\\\\ \\end{aligned}\\] There are two sources of error: random error (unsystematic error) and bias (systematic error) (Figure 12.3). In turn, random error decomposes into generic random error and idiosyncratic random error, whereas bias decomposes into additive systematic error and correlational systematic error (Viswanathan, 2005). Figure 12.3: Random error (unsystematic error) and bias (systematic error). 12.3.3.1 Random error It is any type of error that is inconsistent, or it does not repeat in the same magnitude or direction except by chance. This variability in data collection tends to be averaged out in the long run. Sources of random error could be related to ambiguity, unclear wording, or double-barreled questions. Idiosyncratic random error: It affects a few subjects (e.g., mood, language comprehension). Generic random error: It affects many subjects (e.g., item-wording effects, noisy setting). 12.3.3.2 Bias It is a type of error that consistently affects our measurements. After taking many repeated measures, this systematic error biases the measurement positively or negatively. Luckily, we can predict this type of error. For example, between the first and the second administration of a test, maturation, learning, or fatigue might explain the bias. Additive systematic error: It increases or decreases the observed values by a constant magnitude (e.g., leading questions). Correlational systematic error: It increases or decreases the relationship between variables (e.g., some subjects might interpret the response categories in some way, whereas other subjects might interpret these categories in a different way). "],["scale-development-1.html", "12.4 Scale development", " 12.4 Scale development A valid, reliable, and rigorous psychological instrument needs to comply with some standards that will support its quality. These standards for scale development (Standards for Educational and Psychological Tests, 2014) are published and updated regularly by the American Educational Research Association (AERA), the American Psychological Association (APA), and the National Council on Measurement in Education (NCME). As shown in Table 12.1, the construction of a test is an iterative process aimed at improving the psychometric principles of reliability, validity, comparability, and fairness. These 12 stages of scale development could be grouped into seven main tasks to be completed by the research team: To clearly identify the purpose of the test (e.g., clinical screening) as well as any administration restrictions (e.g., time, administration mode). To define the domain and, based on the domain, create the test specification (i.e., blueprint). To generate an initial pool of items that should be reviewed by a group of experts before beginning the pilot stage. To pre-test the reviewed pool of items using a small sample of respondents to evaluate the psychometric principles of the initial test. To modify the test to achieve higher quality standards after evaluating its psychometric properties. To field test the modified scale using a larger and normative sample (e.g., respondents with a Major Depressive Disorder diagnosis to develop an instrument aimed at this population) to set the final form of the test. To develop guidelines for the administration of the test, the scoring system, and the interpretation of the scores. Table 12.1: Stages of Scale Development Stage Description 1 Purpose identification 2 Administration restrictions 3 Defining the domain 4 Test specification (blueprint) 5 Construct an initial pool of items 6 Review the items 7 Pilot the initial test 8 Modification of test 9 Field test 10 Review the items (final form of test) 11 Develop guidelines for administration, scoring, and interpretation of scores Note. Between Stages 7 and 8, the researcher will analyze the initial pool of items guided by the four psychometric principles to ensure that the scale is reliable, valid, comparable, and fair. Using an iterative approach, the initial pool of items will be sequentially modified until the final form of the scale will be established. 12.4.1 Purpose of the test We must clarify and specify the purpose of the test, the psychological construct that we would like to measure, and what type of decisions will be derived from the test scores. On the one hand, these scores might be used to decide whether one respondent has enough abilities, skills or knowledge on a certain topic or domain (e.g., criterion-referenced tests). On the other hand, test scores might be used to compare subjects of a normative sample for recruitment, psychological assessment, ranking and promotion, classification of people and roles or even clinical screening. 12.4.2 Administration restrictions These restrictions are related to time limit (speed versus power tests), the administration mode (e.g., individually, collectively, computer adaptive testing) and the materials being used (e.g., calculator, lab materials, dictionaries, computers). 12.4.3 Defining the domain To define the domain, we need to think on the set of abilities, knowledge, or traits that we want to measure. These abilities and traits must be related to a specific domain of behaviors and situations that will elicit the latent variables being measured. These behaviors need to reflect the latent variable (i.e., the psychological construct). Psychological constructs are based on theoretical frameworks. Consequently, an in-depth review of the relevant literature will reveal previous attempts to measure the same (or similar) psychological construct. A theory-driven process defining the limits of the psychological construct is recommended. Once the psychological construct has been set, the researcher must specify the behaviors that will reflect the psychological construct as well as the situations and tasks devised to elicit these behaviors. 12.4.4 Test specification (blueprint) To generate items or tasks to measure the behaviors identified when defining the domain, we need to specify all the content areas and the differential weighting of facets. For example, a rubric designed to assess psychology students' knowledge in a module of neuroanatomy and psychophysiology will include items or tasks devised to cover the module's key topics (e.g., brain structures' names and functions, cranial nerve nuclei connections, hormonal pathways). However, not all areas or facets being covered in the test have the same relevance. For this reason, most rubrics include facets with different weightings. For instance, writing a good title or abstract in a quantitative report cannot be considered of equal relevance as writing a good results section. 12.4.5 Draft the initial pool of items, review them, and pilot them The initial pool of items is usually built on the assumption that most of these items won't be included in the final test. It is important to have in mind the purpose of the test before selecting the appropriate response format (e.g., multiple choice, open ended, sorting, identification, correction, filling the gaps, building a response, a presentation). If we decide to use a response format in which respondents have to select their answers from a group of options, then we need to decide how many options to include. For ability tests, the options could range from a true versus false format to several alternatives (ideally no more than 3 response options). In contrast, personality tests could be designed with dichotomous answers (yes versus no) or with graded responses (usually 4 to 9 different points in a quasi-interval scale). Popular graded scales include the pervasive Likert scale, the semantic differential, or the visual analog (Revelle, 2022). Regarding the statements, situations, and behaviors that we evaluate in each item, some rules might apply. Clarity and brevity should always guide scale construction. We must avoid redundancy, ambiguity, and double-barreled items. We need to include some items with a negative wording. These items will require reversing when analyzing the psychometric properties of the scale. The initial pool of items should be reviewed by a group of experts before being administered to a small sample of respondents. These experts will provide very valuable feedback on methodological and theoretical grounds. Once the initial pool of items will be pretested, we will have to conduct analyses to assess the psychometric properties of the items and the test. When piloting the initial pool of items, it is very important to gather qualitative data (e.g., interviews when debriefing the respondents) on instructions' comprehension, item wording and language ambiguity, time constraints, or the effect of respondents' performance. 12.4.6 Psychometric principles: Reliability, validity, comparability, and fairness The reviewed pool of items needs to be pretested on a small sample of respondents to conduct the first quality checks on the psychometric model: reliability, validity, comparability, and fairness. 12.4.6.1 Reliability It accounts for the errors produced during the measurement process. To what extent the observed scores reflect the respondent's true score on the test? Thus, reliability might be related to the repetition and generalization of the measures, the internal consistency of test, or the level of agreement across raters. 12.4.6.2 Validity To what extent the purpose of the test is justified by the nomological network? Are we really measuring what we assume that we are measuring? The psychological construct must be represented in a network of interrelated constructs. We need to examine the network of related psychological constructs and the theories that support the data to test how credible is our proposed model. For doing so, we will use different sources of evidence (e.g., predicted validity, content validity, convergent validity). 12.4.6.3 Comparability Sometimes, different modes of test administration (computerized versus paper-and-pencil) or different experimenters leading the task could produce an unexpected impact on test scores. We need to rule out alternative explanations that could undermine the comparability of respondents' scores. Interestingly, Generalizability Theory (Brennan, 2001), an extension of Classical Test Theory, allows us to decompose the sources of error variance using facets (i.e., conditions of measurement): respondents, tasks, and raters. 12.4.6.4 Fairness The concept of fairness is related to the assumption that the different subsamples being tested do not produce test scores that are biased by sociodemographic differences (e.g., education, gender), language comprehension, or even familiarity with the task. Bias has become a major topic in psychometrics due to the impact of biased high-stakes scales on people's lives. Although there are different forms of bias (e.g., internal, external), item bias has been profusely discussed by psychometricians. There are different approximations to detect item bias (Rust &amp; Golombok, 2009). For example, we can inspect and compare the level of difficulty of each item between two or more groups of respondents (e.g., males versus females). Another approach to item bias relies on comparing the factor analytical structure of different subsamples' scales. 12.4.7 Field test and the development of guidelines for administration, scoring, and interpretation of test scores After checking the psychometric principles of the initial test, an improved version of this test will be administered to a larger and normative sample. The field test will be key to generate the final form of the test. Once the final form of the test will be established, we will write the test guidelines. These guidelines are written for test users interested in administering them in the future. Most of the potential users of the recently developed scale won't be psychometricians or experts in the psychological construct of interest. For this reason, the standards to develop and publish tests, questionnaires, and inventories need to be of a certain quality and clarity (Standards for Educational and Psychological Tests, 2014). The guidelines will include the theoretical framework that supports the psychological construct's specification. Likewise, the guidelines will comprise the purposes of the test, the target population, instructions to administer the test, reliability coefficients and details on the accuracy of the measurement. We will also provide evidence of different types of validity (e.g., criterion validity, convergent validity, construct validity) for the given purposes of the test. Finally, we will provide suggestions on the potential applications of the test (e.g., clinical screening) and the norms to interpret test scores. "],["references-4.html", "12.5 References", " 12.5 References American Educational Research Association, American Psychological Association, &amp; National Council on Measurement in Education (2014). Standards for educational and psychological testing. American Educational Research Association. Brennan, R. L. (2001). Generalizability Theory. Springer Verlag. Krantz, D. H., Luce, R. D., Suppes, P., &amp; Tversky, A. (1971). Foundations of measurement. Vol. 1. Academic Press. Revelle, W. (2022). An introduction to psychometric theory with applications in R. Springer. Retrieved at http://www.personality-project.org/r/book/Chapter2.pdf Rust, J., &amp; Golombok, S. (2009). Modern psychometrics: The science of psychological assessment (3rd ed.). Routledge. Stevens, S. S. (1946). On the theory of scales of measurement. Science, 103, 677—680. Viswanathan, M. (2005). Measurement error and research design. SAGE. "],["exploratory-factor-analysis.html", "Chapter 13 Exploratory Factor Analysis", " Chapter 13 Exploratory Factor Analysis LEARNING OUTCOMES Identify and compare the key features of formative and reflective models. Evaluate the differences between Exploratory Factor Analysis (EFA) and Confirmatory Factor Analysis (CFA). Apply R to tidy and organize the data set before estimating the scale's correlation matrix and generating data visualizations. Apply factor extraction and rotation techniques to generate the scale's simplest factorial structure. Appraise the outputs of the EFA model, interpret the results, and generate plots showing the factorial structure of a scale. Multivariate statistics are a set of statistical techniques aimed at analyzing data when we have many independent variables and/or many dependent variables correlated with one another to varying degrees (Tabachnick &amp; Fidell, 2013). The variables included in our research design might be related via dependence or interdependence models (Hair et al., 2006). Dependence methods attempt to predict the outcome (i.e., dependent variable) using one or several predictors (i.e., independent variables). Thus, some variables will be retained in the final model allowing us to explain, to a certain degree, the outcome. Dependence methods are usually classified as a function of the number of dependent variables being measured (i.e., one dependent variable versus two or more) and the nature of the measurement scales and predictors (i.e., nominal, ordinal, or quantitative data). For instance, to estimate a predictive model when the outcome is one quantitative variable and the explanatory variables are quantitative as well, we use a multiple regression analysis technique (General Linear Model). \\[\\begin{aligned} Y = \\beta_{0} \\ + \\beta_{1}X_{1} \\ + \\beta_{2}X_{2} \\ + \\beta_{3}X_{1}X_{2} + \\epsilon \\\\ \\end{aligned}\\] On the other hand, to estimate a predictive model when we have several quantitative outcomes (i.e., several dependent variables) and the explanatory variables are quantitative as well, we will use a Structural Equation Modeling (SEM) approach. Interdependence methods are aimed at finding the underlying structure of a set of variables, cases, or objects that are analyzed simultaneously. As Table 13.1 shows, these multivariate techniques are usually classified by the nature of the correlated variables (variables, cases, or objects) and by the type of data that we use (quantitative versus categorical data). Table 13.1: Classification of Multivariate Analyses Used in Interdependence Models Relationships among Data Statistical technique Variables Quantitative Principal components Factor analysis Cluster analysis Cases Quantitative Cluster analysis Objects Quantitative Multidimensional scaling Categorical Correspondence analysis Interdependence methods could be used for three main purposes related to specific multivariate techniques: Data reduction: We are interested in the relationship among variables by simplifying the structure of the phenomenon that we study (e.g., job burnout). To do so, we use Principal Components Analysis (PCA) and Factor Analysis (FA). Grouping cases and variables: We are interested in clustering objects and variables by similarity. To accomplish this goal, we use Cluster Analysis. Structure of objects: We are interested in exploring the underlying structure of objects. We use Multidimensional Scaling for quantitative data and Correspondence Analysis for categorical data. "],["data-reduction-techniques.html", "13.1 Data reduction techniques", " 13.1 Data reduction techniques Both Principal Components Analysis (PCA) and Factor Analysis (FA) are data reduction techniques. There is an on-going debate on how to use each method and in which situations (Fabrigar et al., 1999; Velicer &amp; Jackson, 1990). However, there are important differences between these two analytical techniques that should be noted. 13.1.1 Principal Components Analysis (PCA) Principal Components Analysis (PCA) is a method to reduce quantitative and correlated data. It is mainly used to transform a group of correlated variables into a new group of independent variables. It is also used to find linear combinations of the observed variables to generate components that will explain the maximum variability of these original variables. Principal Components Analysis does not need to assume normality. PCA's key feature is that the correlated variables form the components (Figure 13.1). For this reason, these components explain all the variance of the original variables (the sum of the principal components' variances is equal to the sum of the original variables' variances). Consequently, PCA does not estimate the measurement error of the original variables. Figure 13.1: Reflective and formative indicator models. 13.1.2 Factor Analysis (FA) Factor Analysis (FA) is a very popular multivariate technique in the fields of personality, intelligence, social psychology, individual differences, behavioral economics, or even marketing. Although FA is a method that reduces data as PCA does, its popularity lies in its ability to reveal the internal structure (i.e., dimensionality) of tests. In sharp contrast to PCA, FA explains the common or shared variance of the items (i.e., the commonality) (Figure 13.1). Moreover, in PCA we do not estimate the error of the original variables, whereas in FA the variance is decomposed into shared variance (i.e., the variance that the items share due to a common factor) and the unique variance (i.e., the item's error). FA allows us to reduce a set of observed variables or measures (e.g., items in a psychological well-being inventory) to a small set of latent variables (e.g., autonomy, personal growth, positive relationships with others) that are responsible of the behaviors or observed responses provided to those items. In Figure 13.2, the items of the psychological well-being inventory reflect (i.e., are caused by) the six latent variables (first order factors) found after computing an Exploratory Factor Analysis (EFA) or when imposing some theoretical or empirical model to validate it (Confirmatory Factor Analysis or CFA). Figure 13.2: Factorial structure of psychological well-being (Ryff &amp; Keyes, 1995). "],["exploratory-factor-analysis-efa.html", "13.2 Exploratory Factor Analysis (EFA)", " 13.2 Exploratory Factor Analysis (EFA) Exploratory Factor Analysis (EFA) is a type of factor analysis that determines how many latent variables underlie a group of items to explain S (i.e., the covariance matrix) or P (i.e., the correlation matrix). The key feature of EFA is that it explores the underlying structure of a set of measures. For this reason, EFA is considered a key technique when we need to find evidence of internal validity (e.g., the internal structure of a psychological test) (Figure 13.3). Figure 13.3: Exploratory and Confirmatory Factor Analysis models. Some psychological tests and scales are classified as ability or performance tests because their goal is to reliably measure one domain of knowledge to assess the ability of respondents (e.g., an exam to evaluate students' understanding of psychometric principles and analytical techniques). Ability tests assume unidimensionality because we are only interested in measuring one underlying factor (e.g., the student's knowledge of psychometrics). However, if we run an EFA and we discover that our ability test has more than one factor (i.e., it is not unidimensional), we are probably measuring other things (e.g., language comprehension) that will have a negative impact on the ability scores that we intend to measure reliably. Although a key feature of EFA is that it reveals the internal structure of a test, it also identifies which items perform better or worse. We can exclude items that represent a poor fit to the internal structure of the test or that simply are not properly related to the underlying latent variables. Confirmatory Factor Analysis (CFA) EFA explores the latent variables of observed measures, whereas CFA confirms or tests the fit of the observed measures to a particular model or theory (Figure 13.3). The distinctive features of CFA are: CFA assumes the existence of a certain number of common factors and which indicators measure each of these factors, either guided by a theory or by previous \"agnostic\" factor analyses (i.e., EFA) In contrast to EFA, the relationship or the absence of relationship between factors is specified a priori in CFA In CFA, we can correlate unique variances (i.e., items' measurement error) In EFA, we rotate the factors (i.e., orthogonal or oblique rotations) to find the simplest structure, whereas in CFA there is no reason to find the simplest structure as we already know (assume) the optimal factorial structure of the scale 13.2.1 The basic EFA model The basic EFA model can be written as differential scores or using matrix notation. In the first case, we must model a group of p observed variables related to a set of q factors (i.e., latent variables) that explain them plus the error term. \\[\\begin{aligned} Y_{1} = \\lambda_{11}f_{1} \\ + \\lambda_{12}f_{2} \\ + \\ ...\\ + \\lambda_{1q}f_{q} \\ + \\epsilon_{1}\\\\ Y_{2} = \\lambda_{21}f_{1} \\ + \\lambda_{22}f_{2} \\ + \\ ...\\ + \\lambda_{2q}f_{q} \\ + \\epsilon_{2}\\\\ \\vdots\\\\ Y_{p} = \\lambda_{p1}f_{1} \\ + \\lambda_{p2}f_{2} \\ + \\ ...\\ + \\lambda_{pq}f_{q} \\ + \\epsilon_{p}\\\\ \\end{aligned}\\] When using matrix notation, we find four components. Y is the vector with the p observed variables. \\(\\Lambda\\) is the \\(p\\times q\\) matrix that includes the factor loadings (\\(\\lambda_{pq}\\)). f is the vector of factors. Last, \\(\\epsilon\\) is the vector including the error term for each observed variable. \\[Y = \\Lambda \\ f \\ + e\\] The matrix including the factor loadings (\\(\\Lambda\\)) is key to understand the outputs produced by all statistical packages computing factor analysis. Factor loadings (\\(\\lambda_{pq}\\)) are the regression coefficients that show the relative contribution that an observed variable makes to a latent variable or factor. Put differently, factor loadings show us the degree of relationship between each observed variable and the latent variables. \\[\\boldsymbol{\\Lambda} = \\begin{pmatrix} \\lambda_{11} &amp; \\lambda_{12} &amp; \\cdots &amp; \\lambda_{1q} \\\\ \\lambda_{21} &amp; \\lambda_{22} &amp; \\cdots &amp; \\lambda_{2q} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\lambda_{p1} &amp; \\lambda_{p2} &amp; \\cdots &amp; \\lambda_{pq} \\end{pmatrix}\\] 13.2.2 Steps to conduct an Exploratory Factor Analysis (EFA) In the following sections, we will cover the steps that we need to follow to conduct an Exploratory Factor Analysis on a 12-item scale measuring job burnout (Table 13.2). The file containing the data set is named burnout.sav. Table 13.2: Items Measuring Job Burnout Item Statement 1 I feel frustrated by my job 2 I feel emotionally exhausted with my job 3 At the end of the day, I feel rewarded by my employer 4 I do not see prospects of promotion no matter how hard I work 5 I do not feel valued by my boss 6 I feel burned out from my job 7 I do not feel respected by my peers 8 I do not feel fatigued on Monday mornings 9 I never get rewarded when I accomplish my goals 10 Going to work makes me feel stressed 11 Working hard does not pay off 12 I have become more callous toward my peers since I got this job Note. Items 3 and 8 are reverse worded. There are five steps that we need to follow to conduct an EFA: To tidy the data. To generate data visualizations. To extract the appropriate number of latent variables or factors. To rotate the factors to find the simplest factorial structure. To interpret the factorial structure. "],["initial-considerations.html", "13.3 Initial considerations", " 13.3 Initial considerations The scale's observed measures (i.e., the items) must be quantitative data. To estimate the correlations in the structure matrix (P), the scale needs to provide a sufficient amount of variability in the responses. As a rule of thumb, we will need at least 4-5 observed variables per factor and several hundred respondents to avoid underspecified models and a poor estimation of the internal structure of the scale (Fabrigar et al., 1999). Last, we will also require a significant number of high correlations (\\(r &gt; .30\\)) among the items. After collecting the data and selecting the observed variables (i.e., items) that we want to explore, we will organize and tidy the data set. For example, items negatively worded must be recoded. Likewise, we must check if there are missing cases (NaN) to drop them from the analyses. Once the data set will be properly organized and formatted, we will generate the correlation matrix (P) and save it into an R object (e.g., the correlation matrix corre). Then, we will explore our data using some useful plots that will reveal the patterns of association, problematic issues regarding poor quality items, and an initial approximation to the latent variables of the scale. 13.3.1 Tidying the data set In the current example, the Job Burnout Scale was used to measure 510 respondents' degree of agreement (5-point Likert scale: strongly disagree &lt; disagree &lt; neutral &lt; agree &lt; strongly agree) with the 12 statements covering the domain of the psychological construct job burnout. Two items were reversed worded (i3 and i8), requiring further recoding. Starting the session in R: Start every session in R by removing all objects from the global environment using the function rm(). Likewise, set in advance the randomization seed for the current session using the function set.seed(), and avoid using the scientific notation. rm(list = ls()) set.seed(1234) options(scipen=999) Setting the working directory: Place the data set burnout.sav in the same folder where you will set the working directory. Then, set the working directory accordingly. For example, the instructor's working directory can be found in a folder named Chapter13 that is located inside of the folder LDSwR and the folder R; all of them placed in the home folder Users. setwd(&#39;/Users/R/LDSwR/Chapter13&#39;) Loading packages: Load the packages that will be required to execute the R code for this session. library(rio) library(dplyr) library(psych) library(corrplot) library(skimr) Reading the data set: To read the data from the SPSS file stored in the folder datasets inside of the working directory, use the function import() from the package rio but do not forget to add the path with the additional folder (datasets) followed by the name of the data set with its corresponding file extension (e.g., .sav, .csv, .dat). Given that there are 510 observations, it is not recommended to inspect all the rows of the data set. We could use the function head() to inspect the first six participants. We could also explore the dimensions of the data set (e.g., 510 rows and 23 columns) and the names of the variables using the functions dim() and names() respectively. burnout.dat &lt;- import(&#39;./datasets/burnout.sav&#39;) head(burnout.dat) ## id sex age i1 i2 i3 i4 i5 i6 i7 i8 i9 i10 i11 i12 burnoutTot stress job.satis ## 1 1 0 34 4 3 4 2 3 4 3 4 4 4 4 4 34 88 48 ## 2 2 0 38 2 2 4 3 4 2 3 2 3 3 4 3 28 79 53 ## 3 3 1 43 2 5 5 2 4 2 2 5 2 3 3 1 22 82 68 ## 4 4 1 37 3 3 2 4 4 3 3 2 3 4 3 3 33 89 54 ## 5 5 0 43 2 2 4 3 3 1 3 4 3 2 4 2 23 76 51 ## 6 6 1 35 3 3 4 3 3 3 3 3 3 3 4 2 29 83 47 ## coping.skills contract ## 1 71 2 ## 2 82 2 ## 3 56 3 ## 4 64 1 ## 5 73 3 ## 6 74 3 dim(burnout.dat) ## [1] 510 20 names(burnout.dat) ## [1] &quot;id&quot; &quot;sex&quot; &quot;age&quot; &quot;i1&quot; ## [5] &quot;i2&quot; &quot;i3&quot; &quot;i4&quot; &quot;i5&quot; ## [9] &quot;i6&quot; &quot;i7&quot; &quot;i8&quot; &quot;i9&quot; ## [13] &quot;i10&quot; &quot;i11&quot; &quot;i12&quot; &quot;burnoutTot&quot; ## [17] &quot;stress&quot; &quot;job.satis&quot; &quot;coping.skills&quot; &quot;contract&quot; Subsetting: To generate the correlation or structure matrix (P), we only need the 12 items of the Job Burnout Scale. For this reason, we are going to subset the data set to select the items 1 to 12 using the function select() from the package dplyr. The resulting 12 covariates will be stored in a new R object named burnout. burnout &lt;- burnout.dat %&gt;% select(i1:i12) headTail(burnout) ## i1 i2 i3 i4 i5 i6 i7 i8 i9 i10 i11 i12 ## 1 4 3 4 2 3 4 3 4 4 4 4 4 ## 2 2 2 4 3 4 2 3 2 3 3 4 3 ## 3 2 5 5 2 4 2 2 5 2 3 3 1 ## 4 3 3 2 4 4 3 3 2 3 4 3 3 ## ... ... ... ... ... ... ... ... ... ... ... ... ... ## 507 2 2 2 4 2 2 2 4 3 2 3 4 ## 508 1 3 5 2 2 1 3 4 3 4 1 2 ## 509 2 4 2 4 4 2 4 4 5 4 3 4 ## 510 4 4 4 4 4 2 4 2 5 4 5 2 dim(burnout) ## [1] 510 12 names(burnout) ## [1] &quot;i1&quot; &quot;i2&quot; &quot;i3&quot; &quot;i4&quot; &quot;i5&quot; &quot;i6&quot; &quot;i7&quot; &quot;i8&quot; &quot;i9&quot; &quot;i10&quot; &quot;i11&quot; &quot;i12&quot; Reverse scoring: We need to reverse the scoring of the two items that were negatively worded (i3 and i8). First, we will create a new vector (new) after selecting all the rows and only the columns of the items named i3 and i8. The vector named new is the result of subtracting a value of 6 to the scores of two items that range from 1 to 5. Consequently, a score of 1 will be transformed into a score of 5 (i.e., 6 - 1 = 5) and a score of 5 will be transformed into a score of 1 (i.e., 6 - 5 = 1). After reverse scoring the two items included in the vector new, we will substitute the original items i3 and i8 included in our data set with the new ones stored in the R object new. As usual, you can inspect the result using the function head(). new &lt;- 6 - burnout[ , c(&#39;i3&#39;, &#39;i8&#39;)] burnout[ , c(&#39;i3&#39;, &#39;i8&#39;)] &lt;- new head(burnout) ## i1 i2 i3 i4 i5 i6 i7 i8 i9 i10 i11 i12 ## 1 4 3 2 2 3 4 3 2 4 4 4 4 ## 2 2 2 2 3 4 2 3 4 3 3 4 3 ## 3 2 5 1 2 4 2 2 1 2 3 3 1 ## 4 3 3 4 4 4 3 3 4 3 4 3 3 ## 5 2 2 2 3 3 1 3 2 3 2 4 2 ## 6 3 3 2 3 3 3 3 3 3 3 4 2 13.3.2 Generating the correlation matrix To conduct EFA, we can use the covariance or the correlation matrices. Here, we will use the correlation matrix (P; also called structure matrix). The function cor() generates the correlation matrix that we will store in the R object corre. In the first argument of the function cor(), we will input the data set that only includes the items. Although the second argument allows us to select among three different correlation methods (pearson, kendall, and spearman), the default option (pearson) is the one we need. We will use the function na.omit() to avoid having missing observations. If these missing cases are not explicitly excluded from the analyses, we won't be able to generate the correlation matrix. To inspect the correlation matrix in the console simply recall the R object corre. The correlation matrix displayed below was produced with the package stargazer using the function stargazer() (Hlavac, 2018). CAUTION! We used the function stargazer() from the package stargazer within the Rmarkdown environment to produce a more appealing correlation matrix to the reader. It is important to set the argument results = 'asis' in the Rmarkdown code chunk to generate text as raw Markdown content. corre &lt;- cor(na.omit(burnout)) stargazer::stargazer(corre, type = &#39;html&#39;, rownames = T, summary = F, digits = 2, header = F, font.size = &#39;Large&#39;, column.sep.width = &#39;7pt&#39;, align = T, title = &quot;Job Burnout Scale&#39;s Correlation Matrix.&quot;) Job Burnout Scale's Correlation Matrix. i1 i2 i3 i4 i5 i6 i7 i8 i9 i10 i11 i12 i1 1 0.47 0.28 0.31 0.30 0.58 0.20 0.46 0.23 0.46 0.03 0.24 i2 0.47 1 0.33 0.31 0.38 0.37 0.27 0.31 0.26 0.43 0.13 0.24 i3 0.28 0.33 1 0.45 0.22 0.40 0.31 0.27 0.34 0.28 0.07 0.13 i4 0.31 0.31 0.45 1 0.27 0.46 0.37 0.19 0.44 0.27 0.19 0.08 i5 0.30 0.38 0.22 0.27 1 0.31 0.41 0.20 0.45 0.44 0.26 0.19 i6 0.58 0.37 0.40 0.46 0.31 1 0.24 0.38 0.30 0.40 0.11 0.13 i7 0.20 0.27 0.31 0.37 0.41 0.24 1 0.06 0.68 0.23 0.41 0.11 i8 0.46 0.31 0.27 0.19 0.20 0.38 0.06 1 0.11 0.45 -0.11 0.23 i9 0.23 0.26 0.34 0.44 0.45 0.30 0.68 0.11 1 0.27 0.41 0.15 i10 0.46 0.43 0.28 0.27 0.44 0.40 0.23 0.45 0.27 1 0.06 0.31 i11 0.03 0.13 0.07 0.19 0.26 0.11 0.41 -0.11 0.41 0.06 1 0.04 i12 0.24 0.24 0.13 0.08 0.19 0.13 0.11 0.23 0.15 0.31 0.04 1 After generating the correlation matrix stored in corre, we will compute the Kaiser-Meyer-Olkin (KMO) test of factorial adequacy. To have a structure matrix appropriate for factor analysis, the overall KMO index should be greater than .80 and never below .70. The overall measure of sampling adequacy (MSA) was .85. Consequently, we can proceed with the factor analysis. KMO(corre) ## Kaiser-Meyer-Olkin factor adequacy ## Call: KMO(r = corre) ## Overall MSA = 0.85 ## MSA for each item = ## i1 i2 i3 i4 i5 i6 i7 i8 i9 i10 i11 i12 ## 0.84 0.90 0.87 0.87 0.89 0.85 0.80 0.85 0.80 0.88 0.82 0.87 13.3.3 Exploring the data using descriptive statistics and data visualizations The function skim() from the package skimr provides a good summary of the items (i.e., the location, dispersion, and the form of the distribution). skim(burnout) Table 13.3: Data summary Name burnout Number of rows 510 Number of columns 12 _______________________ Column type frequency: numeric 12 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist i1 0 1 2.73 1.12 1 2 3 4 5 ▃▇▅▅▁ i2 0 1 3.36 1.08 1 2 4 4 5 ▁▅▅▇▃ i3 0 1 2.92 1.36 1 2 3 4 5 ▅▇▃▇▅ i4 0 1 3.10 1.25 1 2 3 4 5 ▃▆▅▇▃ i5 0 1 3.29 1.09 1 2 4 4 5 ▁▃▃▇▂ i6 0 1 2.46 1.15 1 2 2 3 5 ▅▇▃▃▁ i7 0 1 3.34 1.11 1 3 3 4 5 ▁▅▆▇▃ i8 0 1 2.78 1.19 1 2 2 4 5 ▂▇▃▅▂ i9 0 1 3.64 1.03 1 3 4 4 5 ▁▂▅▇▃ i10 0 1 3.57 0.99 1 3 4 4 5 ▁▂▃▇▂ i11 0 1 3.71 1.12 1 3 4 5 5 ▁▂▃▇▅ i12 0 1 3.63 1.04 1 3 4 4 5 ▁▂▃▇▃ The function skim_with() from the package skimr enables us to specify our preferred list of descriptive statistics. We can also set the names of those statistics (e.g., we can display the APA abbreviation Mdn instead of median) or add other relevant descriptive statistics such as the Median Absolute Deviation (MAD). my.summary &lt;- skim_with(numeric = sfl(M = mean, SD = sd, Mdn = median, MAD = mad, Histogram = inline_hist), append = F) my.summary(burnout) Table 13.4: Data summary Name burnout Number of rows 510 Number of columns 12 _______________________ Column type frequency: numeric 12 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate M SD Mdn MAD Histogram i1 0 1 2.73 1.12 3 1.48 ▃▇▁▅▁▅▁▁ i2 0 1 3.36 1.08 4 1.48 ▁▅▁▅▁▇▁▃ i3 0 1 2.92 1.36 3 1.48 ▅▇▁▃▁▇▁▅ i4 0 1 3.10 1.25 3 1.48 ▃▆▁▅▁▇▁▃ i5 0 1 3.29 1.09 4 1.48 ▁▃▁▃▁▇▁▂ i6 0 1 2.46 1.15 2 1.48 ▅▇▁▃▁▃▁▁ i7 0 1 3.34 1.11 3 1.48 ▁▅▁▆▁▇▁▃ i8 0 1 2.78 1.19 2 1.48 ▂▇▁▃▁▅▁▂ i9 0 1 3.64 1.03 4 1.48 ▁▂▁▅▁▇▁▃ i10 0 1 3.57 0.99 4 1.48 ▁▂▁▃▁▇▁▂ i11 0 1 3.71 1.12 4 1.48 ▁▂▁▃▁▇▁▅ i12 0 1 3.63 1.04 4 1.48 ▁▂▁▃▁▇▁▃ We use data visualization techniques to inspect relationships and patterns of association among items. For example, the function cor.plot() from the package psych (Revelle, 2018) shows a heatmap with red colors related to negative correlations and blue colors aimed at displaying the positive correlations. The functions corrplot.mixed() from the package corrplot or pairs.panels() from the package psych provide a lot of possibilities to visualize the correlation matrix stored in our R object named corre (see Figures 13.4, 13.5, and 13.6). cor.plot(corre) Figure 13.4: A correlation plot with heatmap. You can add and modify as many arguments as you wish. For example, in the function corrplot.mixed(), number.cex reveals the character expansion value that enables the modification of the size of the numbers appearing in the plot, whereas tl.cex changes the size of the text labels (i.e., the names of the items that are displayed on the main diagonal of the correlation plot). corrplot.mixed(corre, number.cex = 0.95, tl.col = &#39;orange4&#39;, tl.cex = 1.3, upper = &#39;square&#39;) Figure 13.5: Mixed methods to visualize a correlation matrix. The function pairs.panels() displays a scatter plot of matrices (SPLOM), with histograms on the main diagonal, and bivariate scatter plots and Pearson's correlations below and above the main diagonal. This plot works well with a small number of covariates. pairs.panels(corre, hist.col = &#39;#00AFBB&#39;, cex.cor = 1.10, cex.labels = 0.90, upper = &#39;square&#39;, lm = TRUE, show.points = TRUE, stars = TRUE, density = T, ci = TRUE) Figure 13.6: A scatter plot of matrices (SPLOM), with bivariate scatter plots, histograms, and Pearson's correlation. "],["factor-extraction.html", "13.4 Factor extraction", " 13.4 Factor extraction The first step in EFA is to determine how many latent variables we should retain for a given scale. There are different methods to extract the appropriate number of factors. Here, we will cover how to extract factors using the parallel analysis as it is a better factor extraction method than popular alternatives such as the eigenvalues approach and the scree test. Parallel analysis is the most robust factor extraction method so far (Horn, 1965). The rationale of parallel analysis is to simulate a data set under similar conditions (e.g., same number of respondents and items). For doing so, the algorithm extracts thousand random samples that are going to be compared to the actual data. The number of factors to retain is shown by where the tracings for actual and simulated data cross using the PC method (Figure 13.7). In our example, we should extract only two factors because those are the eigenvalues located above the red line of the simulated data that crosses the blue line of the actual data under the principal components method (PC). We have set the number of observations (our sample size; N = 510) as the second argument of the function fa.parallel(). fa.parallel(corre, n.obs = 510) Figure 13.7: Parallel analysis scree plots. "],["factor-rotation-and-interpretation-of-efa.html", "13.5 Factor rotation and interpretation of EFA", " 13.5 Factor rotation and interpretation of EFA To perform EFA we will use the function fa() from the package psych. This function allows us to select the factoring method for the extraction of factors (e.g., minimum residual, maximum likelihood, weight least squares). We will use ml because the maximum likelihood algorithm finds the communality values that minimize the chi-square goodness of fit, assuming normality in continuous indicators. We must specify the number of factors to extract (in our example, two factors). We can enter this argument (nfactors = 2) because we have computed a parallel analysis in the previous section. The last important argument to set is the factor rotation (rotate). Why do we rotate factors? The main reason to rotate factors is to favor its interpretation by producing the simplest structure of the factor loadings. There are two types of factor rotations: orthogonal and oblique rotations (Figure 13.8). The orthogonal rotation assumes that factors are independent (i.e., uncorrelated). Consequently, the axes remain perpendicular as they rotate. In psychology, it is very uncommon to deal with independent factors. However, there is considerable amount of evidence supporting the orthogonal and bidimensional structure of affect (i.e., core affect) (Russell, 1980, 2003). For orthogonal rotations, varimax and quartimax are the most common methods. On the other hand, oblique rotations allow factors to correlate. Consequently, the axes are not perpendicular when rotated. We prefer using the oblique rotation oblimin instead of other popular methods such as promax and quartimin. Figure 13.8: Orthogonal and oblique rotations. burnout.fa &lt;- fa(corre, nfactors = 2, rotate = &#39;oblimin&#39;, scores = &#39;regression&#39;, fm = &#39;ml&#39;) summary(burnout.fa) ## ## Factor analysis with Call: fa(r = corre, nfactors = 2, rotate = &quot;oblimin&quot;, scores = &quot;regression&quot;, ## fm = &quot;ml&quot;) ## ## Test of the hypothesis that 2 factors are sufficient. ## The degrees of freedom for the model is 43 and the objective function was 0.34 ## ## The root mean square of the residuals (RMSA) is 0.05 ## The df corrected root mean square of the residuals is 0.06 ## ## With factor correlations of ## ML2 ML1 ## ML2 1.00 0.37 ## ML1 0.37 1.00 print(burnout.fa, sort = TRUE) ## Factor Analysis using method = ml ## Call: fa(r = corre, nfactors = 2, rotate = &quot;oblimin&quot;, scores = &quot;regression&quot;, ## fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## item ML2 ML1 h2 u2 com ## i1 1 0.76 -0.04 0.56 0.44 1.0 ## i8 8 0.68 -0.18 0.40 0.60 1.1 ## i6 6 0.65 0.10 0.48 0.52 1.0 ## i10 10 0.64 0.06 0.44 0.56 1.0 ## i2 2 0.56 0.12 0.38 0.62 1.1 ## i3 3 0.37 0.25 0.27 0.73 1.7 ## i12 12 0.32 0.03 0.11 0.89 1.0 ## i9 9 0.04 0.83 0.71 0.29 1.0 ## i7 7 -0.01 0.80 0.64 0.36 1.0 ## i11 11 -0.16 0.56 0.28 0.72 1.2 ## i5 5 0.30 0.40 0.35 0.65 1.9 ## i4 4 0.33 0.38 0.34 0.66 2.0 ## ## ML2 ML1 ## SS loadings 2.76 2.19 ## Proportion Var 0.23 0.18 ## Cumulative Var 0.23 0.41 ## Proportion Explained 0.56 0.44 ## Cumulative Proportion 0.56 1.00 ## ## With factor correlations of ## ML2 ML1 ## ML2 1.00 0.37 ## ML1 0.37 1.00 ## ## Mean item complexity = 1.3 ## Test of the hypothesis that 2 factors are sufficient. ## ## The degrees of freedom for the null model are 66 and the objective function was 3.74 ## The degrees of freedom for the model are 43 and the objective function was 0.34 ## ## The root mean square of the residuals (RMSR) is 0.05 ## The df corrected root mean square of the residuals is 0.06 ## ## Fit based upon off diagonal values = 0.98 ## Measures of factor score adequacy ## ML2 ML1 ## Correlation of (regression) scores with factors 0.91 0.92 ## Multiple R square of scores with factors 0.83 0.84 ## Minimum correlation of possible factor scores 0.66 0.69 The results from the standardized loadings matrix are telling. The proportion of communality of one item regarding the latent variables is shown in the column \\(h^2\\), whereas \\(u^2\\) represents the uniqueness (the error term). The column showing the complexity (com) is illustrative of items whose factor loadings do not contribute primarily to one factor. For example, items 4 and 5 (i.e., i4 and i5) yielded the highest complexity values, with large factor loadings in both factors. We must inspect the factor loadings of each item. Factor loadings greater than .30 (\\(\\lambda^2 &gt; .30\\)) represent good loadings. However, as we have seen with the index of complexity, factor loadings greater than .30 in at least two factors indicate that the item is problematic (e.g., i4). After inspecting the items with factor loadings greater than the established cut-off point (\\(\\lambda^2 &gt; .30\\)) on one factor only, we must provide a label for each latent variable. This process is done inductively, and it is guided by the theory. For example, the items with high factor loadings in factor 2 (ML2) could be appraised as Affective items (i.e., items that cover the subdomain of affective burnout). On the other hand, the items that are a good reflection of factor 1 (ML1) could be related to a Reward component of job burnout. After labeling the factors, we will substitute (rename) the names of the columns of the list containing the factor loadings (ML2 and ML1) with the new names (Affective and Reward). colnames(burnout.fa$loadings) ## [1] &quot;ML2&quot; &quot;ML1&quot; colnames(burnout.fa$loadings) &lt;- c(&#39;Affective&#39;, &#39;Reward&#39;) There is a better way to inspect our results if we use the function print(). We can specify the optional arguments cutoff (i.e., it only displays factor loadings greater than the cut-off point set at .30) and sort (e.g., if TRUE, all factor loadings are ordered). print(loadings(burnout.fa), cutoff = .3, digits = 2, sort = TRUE) ## ## Loadings: ## Affective Reward ## i1 0.76 ## i2 0.56 ## i6 0.65 ## i8 0.68 ## i10 0.64 ## i7 0.80 ## i9 0.83 ## i11 0.56 ## i3 0.37 ## i4 0.33 0.38 ## i5 0.30 0.40 ## i12 0.32 ## ## Affective Reward ## SS loadings 2.65 2.07 ## Proportion Var 0.22 0.17 ## Cumulative Var 0.22 0.39 It seems that items 4 and 5 are problematic. These items should be dropped from the scale. Thus, we should re-run the EFA with the new subset of items (i.e., 10 items only). This process is iterative, and it will require a data-driven approach as it is an exploratory analysis. To plot the results, we can generate a path diagram that will display the factorial structure of the Job Burnout Scale (Figure 13.9). The function fa.diagram() from the package psych can be customized to modify parameters such as the size of the ellipses surrounding the latent variables (e.size) or the squares of the observed variables (rsize). We can also set the cut-off point of the factor loadings to .30 or any other value to avoid displaying factor loadings smaller than the threshold set with the argument cut. fa.diagram(burnout.fa, main = &#39;\\n Exploratory Factor Analysis (Job Burnout Scale)&#39;, labels = names(burnout), l.cex = 2, digits = 2, e.size = .05, rsize = 2.13, cut = .30, cex = 1.2, adj = 2) Figure 13.9: Path diagram (Job Burnout Scale). "],["references-5.html", "13.6 References", " 13.6 References Fabrigar, L. R., Wegener, D. T., MacCallum, R. C., &amp; Strahan, E. J. (1999). Evaluating the use of exploratory factor analysis in psychological research. Psychological Methods, 4(3), 272—299. Hair, J. F., Black, W. C., Babin, B. J., Anderson, R. E., &amp; Tatham, R. L. (2006). Multivariate data analysis (6th ed.). Pearson Prentice Hall. Hlavac, M. (2018). stargazer: Well-formatted regression and summary statistics tables. Retrieved at https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf Horn, J. (1965) A rationale and test for the number of factors in factor analysis. Psychometrika, 30, 179—185. Revelle, W. (2018). psych: Procedures for psychological, psychometric, and personality research. Retrieved at http://personality-project.org/r/psych/psych-manual.pdf Russell, J. A. (1980). A circumplex model of affect. Journal of Personality and Social Psychology, 39, 1161—1178. Russell, J. A. (2003). Core affect and the psychological construction of emotion. Psychological Review, 110, 145—172. Ryff, C. D., &amp; Keyes, C. L. M. (1995). The structure of psychological well-being revisited. Journal of Personality and Social Psychology, 69(4), 719—727. Tabachnick, B. G., &amp; Fidell, L. S. (2014). Using Multivariate Statistics (6th ed.). Pearson. Velicer, W. F., &amp; Jackson, D. N. (1990). Component analysis versus common factor analysis: Some issues in selecting the appropriate procedure. Multivariate Behavioral Research, 25, 1—28. "],["reliability-1.html", "Chapter 14 Reliability", " Chapter 14 Reliability LEARNING OUTCOMES Identify the different approaches to the concept of reliability. Distinguish the key features of reliability for several administrations of a test, inter-rater reliability, and internal consistency. Apply R to compute reliability coeﬀicients of internal consistency and to generate data visualizations. Critically evaluate the different reliability coefficients and appraise the outputs produced with the functions included in the package psych. Despite being a common-sense concept (i.e., how reliable and consistent is the measurement instrument?), reliability has generated much debate among psychometricians. Reliability is conceptualized as the proportion of observed score variance attributed to true score variance. \\[\\begin{aligned} \\ Reliability = \\frac{\\sigma^{2}_{T}}{\\sigma^{2}_{X}} \\end{aligned}\\] Although the concept of reliability as the ratio between the true and the observed scores' variability is easy to understand, the different ways to operationalize reliability and to estimate the diversity of reliability coefficients complicate things a little bit (Cronbach, 1951; DeVellis, 2017; Huysamen, 2006; Revelle &amp; Zinbarg, 2009; Sijtsma, 2009). In Classical Test Theory (CTT), test scores are based on the general linear model and test total scores (Crocker &amp; Algina, 1986). This measurement theory is also called True Score Theory because it decomposes the observed score into true scores and measurement error (random error and bias). \\[\\begin{aligned} \\ X_{i} = \\ T_{i} + \\ E_{i} \\end{aligned}\\] Derived from the former psychometric model, CTT includes four additional assumptions: The expected value of the observed score is the true score. \\[\\begin{aligned} \\ E(X_{i}) = \\ T_{i} \\end{aligned}\\] True scores and errors are independent. \\[\\begin{aligned} \\ Cov(T_{i}, \\ E_{i}) = \\ 0 \\end{aligned}\\] Errors across test's forms are independent. \\[\\begin{aligned} \\ Cov(E_{i}, \\ E_{j}) = \\ 0 \\end{aligned}\\] The errors in one form are independent of the true score on another form. \\[\\begin{aligned} \\ Cov(E_{i}, \\ T_{j}) = \\ 0 \\end{aligned}\\] In sum, the assumptions of the CTT model consider that the variance components are independent. \\[\\begin{aligned} \\sigma^{2}_{X} = \\sigma^{2}_{T} + \\sigma^{2}_{E} \\end{aligned}\\] "],["approaches-to-reliability.html", "14.1 Approaches to reliability", " 14.1 Approaches to reliability Reliability can be operationalized in different ways (Table 14.1). First, we could consider reliability as an estimation of measurement stability. Consequently, we should administer the same test several times. Second, we might consider reliability as the estimation of measurement equivalence in two equal forms. To estimate this type of reliability we must create two equal forms (i.e., alternate forms) of the same test. Third, we could consider reliability as an estimation of consistency across raters. Then, the same task (e.g., classifying objects or people) should be applied to two or more raters/judges. Last, reliability can be conceptualized as a method to estimate the internal consistency of a test. In this case, we need to administer the same test to at least two respondents to generate several scores on the same test. Table 14.1: Approaches to Reliability Estimation Coefficient Operationalization Test administration Source of error Test-retest \\(Cor(X_{1}, X_{2})\\) Repetition of measurement Two or more times Instability over time Parallel forms \\(Cor(X_{i}, X_{j})\\) Two alternate forms Twice Lack of equivalence Inter-rater Once Disagreement among raters Cohen-Kappa (\\(\\kappa\\)) Two raters (categorical data) Fleiss-Kappa More than two raters (categorical data) Intraclass correlation Two or more raters (quantitative data) Internal consistency Two or more scores in a single test Once Heterogeneous item content Split half tests Spearman-Brown Tau Equivalent Item covariance Alpha (\\(\\alpha\\)) KR--20 Guttman's Lambdas (\\(\\lambda\\)) Omega (\\(\\omega_{h}, \\omega_{t}\\)) EFA/CFA Note. \\(Cor(X_{1}, X_{2})\\) = Correlation between test and retest. \\(Cor(X_{i}, X_{j})\\) = Correlation between forms \\(i\\) and \\(j\\) of the test. KR--20 = Kuder-Richardson Formula 20 to estimate the reliability coefficient for binary data. EFA/CFA = Exploratory Factor Analysis/Confirmatory Factor Analysis. "],["reliability-methods-requiring-several-administrations-of-a-test.html", "14.2 Reliability methods requiring several administrations of a test", " 14.2 Reliability methods requiring several administrations of a test 14.2.1 Test-retest The test-retest method operationalizes reliability as stability of the measures over time. For example, the same test could be administered at the beginning and at the end of the academic year. If the instrument is reliable, the time interval between test administrations should have no effect on test scores. To estimate the reliability of a test-retest, we will compute a Pearson's correlation test between the test and the retest's scores. The estimation of this type of reliability depends on the measurement scale. For instance, for ordinal data we will compute Kendall's Tau-b tests and for binary data we will compute tetrachoric correlations. The method to estimate reliability as stability over time is frequently used in speed tests. However, this method can be negatively affected by the time interval between the test and retest (e.g., long periods are related to respondents' individual changes) as well as the influence of the first administration on the second (e.g., recall effects, practice). 14.2.2 Parallel forms The parallel-forms method is designed to overcome some of the limitations of the test-retest approach. For instance, a test aimed at measuring the respondents' ability in verbal comprehension---if administered twice---might be affected by learning and memory effects. For this reason, we could develop two parallel (alternate) forms under the same domain and test specification. These two forms should be administered at the same time and to a large number of respondents. Finally, we should compute Pearson's correlation test to the scores of both parallel forms. This method assumes equal true scores and error variances of both alternate forms. \\[\\begin{aligned} \\ X_{i} = \\ T_{i} + \\ E_{i} \\end{aligned}\\] \\[\\begin{aligned} \\ X&#39;_{i} = \\ T_{i} + \\ E&#39;_{i} \\end{aligned}\\] \\[\\begin{aligned} \\sigma^{2}_{e} = \\sigma^{2}_{e&#39;} \\end{aligned}\\] In sum, the reliability coefficient of a parallel-forms test will be estimated as the Pearson's correlation of two parallel forms (i.e., two parallel tests). \\[\\begin{aligned} \\rho_{xx&#39;} = \\frac{\\sigma^{2}_{T}}{\\sigma^{2}_{X}} \\end{aligned}\\] \\[\\begin{aligned} \\sigma^{2}_{e} = \\sigma^{2}_{x} \\ \\sqrt{1 - \\rho_{xx&#39;}} \\end{aligned}\\] CAUTION! This method is problematic because it is highly unlikely to find strict parallelism between two forms. The parallel-forms reliability coefficient includes random errors and the degree of parallelism of the forms. The parallel-forms method is affected by practice effects. "],["inter-rater-reliability.html", "14.3 Inter-rater reliability", " 14.3 Inter-rater reliability Inter-rater reliability estimates the consistency among raters/judges (i.e., the agreement among raters). There are several methods to estimate inter-rater reliability. We can organize these methods as a function of the number of raters and the level of measurement. When we want to test the inter-rater reliability of two observers classifying objects or persons (i.e., nominal data), Cohen's Kappa (\\(\\kappa\\)) would be the most appropriate method (Cohen, 1960). However, Fleiss's Kappa will be the preferred option for nominal data and more than two raters. With ordinal data and two raters, estimating a Weight Kappa coefficient is recommended. On the other hand, Kendall's Tau-b test is the most appropriate test statistic to estimate the agreement of ordered objects or persons for multiple raters. For quantitative data (e.g., interval data), the intraclass correlation (ICC) is the best method to measure the relative homogeneity of scores within a class (e.g., persons). The ICC estimates reliability using an Analysis of Variance (ANOVA) approach. 14.3.1 Cohen's Kappa Cohen's Kappa (\\(\\kappa\\)) estimates the degree of agreement between two raters/judges when assessing and classifying objects or persons (e.g., classification and diagnosis of mental disorders). This method is highly recommended because it considers the effect of random agreement. The agreement between two judges cannot be estimated by only counting the number of agreed cases (i.e., the main diagonal of a confusion matrix). We also need to subtract the estimation of the random agreement based on the marginal cross-products divided by the total number of squared observations. To compute the observed agreement (\\(p_{o}\\)), we sum all the values from the main diagonal divided by the total number of observations. \\[\\begin{aligned} \\ p_{o} = \\displaystyle\\sum_{j=1}^{m} p_{jj} = \\frac{N_{11}} {N} \\ + \\ \\frac{N_{22}} {N} \\ + \\ \\dots \\ + \\frac{N_{mm}} {N} \\end{aligned}\\] To estimate the agreement due to randomness (\\(p_{e}\\)), we sum the marginal cross-products divided by the total number of squared observations. \\[\\begin{aligned} \\ p_{e} = \\displaystyle\\sum_{j=1}^{m} p_{jje} = \\displaystyle\\sum_{j=1}^{m} \\frac{\\ {N_{i}} \\times {N_{j}}} {{N^{2}}} \\end{aligned}\\] Cohen's Kappa (\\(\\kappa\\)) is computed by subtracting the agreement due to randomness (\\(p_{e}\\)) to the observed agreement (\\(p_{o}\\)) divided by 1 minus the agreement due to randomness (\\(p_{e}\\)). The final inter-rater agreement coefficient will range from 0 to 1. Values in the .80 — 1 range are considered very good inter-rater reliability coefficients. Kappa's coefficients in the .60 — .79 range represent acceptable levels of agreement. However, agreement rates below .60 are usually considered unacceptable. \\[\\begin{aligned} \\ K = \\frac{\\ p_{o} \\ - \\ p_{e}} {1 \\ - \\ p_{e}} \\end{aligned}\\] 14.3.2 Cohen's Kappa in R We will use the functions Kappa() and agreementplot() from the package vcd to compute Cohen's Kappa (\\(\\kappa\\)) coefficients and to generate agreement plots for two different simulated data sets (Friendly, 2017). library(vcd) The following simulated data sets will show the level of agreement between two clinical psychologists assessing and ascribing affective disorders to a group of 220 patients. Bi1 means Bipolar I disorder. Bi2 means Bipolar II disorder. PD means Persistent depressive. C means Cyclothymic. MD means Major depressive. We will generate two different matrices in which the observations (i.e., number of patients per cell) will be distributed along the main diagonal (i.e., agreement between raters) and off the main diagonal (i.e., disagreement between raters). The first data set yields an inter-rater reliability of \\(\\kappa = .46, \\ 95\\% \\ CI{[.38, \\ .54]}\\), that should be considered unacceptable. The function agreementplot() generates a representation of a k by k confusion matrix, where the observed and expected diagonal elements are represented by superposed black and white rectangles, respectively. dat1 &lt;- matrix(c(20, 27, 1, 3, 0, 23, 21, 0, 0, 0, 4, 2, 46, 1, 0, 2, 2, 0, 14, 17, 0, 0, 0, 13, 24), 5, 5, byrow = T, dimnames= list(c(&#39;Bi1&#39;, &#39;Bi2&#39;, &#39;PD&#39;, &#39;C&#39;, &#39;MD&#39;), c(&#39;Bi1&#39;, &#39;Bi2&#39;, &#39;PD&#39;, &#39;C&#39;, &#39;MD&#39;))) K1 &lt;- Kappa(dat1) print(K1, CI = TRUE) ## value ASE z ## Unweighted 0.4574 0.04169 10.97 ## Weighted 0.6774 0.02876 23.55 ## Pr(&gt;|z|) ## Unweighted 0.00000000000000000000000000051798280819831024980496793930887936602748847277526847309482233086705226356508124929689529380993918 ## Weighted 0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001148 ## lower upper ## Unweighted 0.3757 0.5391 ## Weighted 0.6210 0.7337 agreementplot(dat1, ylab = &#39;Rater 1&#39;, ylab_rot = 90, ylab_just = &#39;center&#39;, xlab = &#39;Rater 2&#39;, xlab_rot = 0, xlab_just = &#39;center&#39;, reverse_y = TRUE, weights = 1) Figure 14.1: Agreement plot displaying low agreement between two clinical psychologists assessing individuals with affective disorders. The second data set yields a very high inter-rater reliability, \\(\\kappa = .91, \\ 95\\% \\ CI {[.87, \\ .95]}\\). The reason for such a high Kappa coefficient can be found in the small amount of disagreement between raters (i.e., there are very few cases in the off-diagonal cells). dat2 &lt;- matrix(c(79, 0, 0, 0, 0, 2, 31, 0, 0, 0, 1, 3, 46, 1, 0, 0, 2, 0, 6, 0, 0, 1, 0, 5, 43), 5, 5, byrow = T, dimnames= list(c(&#39;Bi1&#39;, &#39;Bi2&#39;, &#39;PD&#39;, &#39;C&#39;, &#39;MD&#39;), c(&#39;Bi1&#39;, &#39;Bi2&#39;, &#39;PD&#39;, &#39;C&#39;, &#39;MD&#39;))) K2 &lt;- Kappa(dat2) print(K2, CI = TRUE) ## value ASE z Pr(&gt;|z|) lower upper ## Unweighted 0.9087 0.02239 40.58 0 0.8648 0.9526 ## Weighted 0.9457 0.01498 63.15 0 0.9163 0.9750 agreementplot(dat2, ylab = &#39;Rater 1&#39;, ylab_rot = 90, ylab_just = &#39;center&#39;, xlab = &#39;Rater 2&#39;, xlab_rot = 0, xlab_just = &#39;center&#39;, reverse_y = TRUE, weights = 1) Figure 14.2: Agreement plot displaying high agreement between two clinical psychologists assessing individuals with affective disorders. "],["internal-consistency.html", "14.4 Internal consistency", " 14.4 Internal consistency The problems found when trying to generate parallel forms are avoided when using internal consistency methods. The basic assumption is to consider the observed variables of a test (i.e., items) as the sample of observations drawn from a specific domain. A way to estimate the internal consistency of the test is to inspect the consistency of respondents in different parts of the test. To do so, we can divide the test in two halves (Split half tests) or use the information of all items (e.g., Cronbach's alpha). 14.4.1 Split half tests In this method, all respondents complete the test. Then, the researcher splits the observed variables (e.g., items) in two halves. This procedure requires parallelism of both halves. The goal is to correlate the group of scores from both halves (\\(\\rho_{12}\\)). The famous Spearman-Brown correction formula for two halves is used to estimate the reliability coefficient. \\[\\begin{aligned} \\rho_{xx&#39;} = \\frac{\\ 2 \\rho_{12}}{1 + \\rho_{12}} \\end{aligned}\\] A more liberal approach is used when we only keep the assumption of true scores' equal variances. These methods are called Tau Equivalent reliability methods. These estimations (e.g., Rulon formula) provide smaller values than computations using Spearman-Brown's equation. \\[\\begin{aligned} \\rho_{xx&#39;} = 2\\Bigg( 1 \\ - \\frac{ \\sigma^{2}_{x_{1}} \\ + \\sigma^{2}_{x_{2}}} {\\sigma^{2}_{x}}\\Bigg) \\end{aligned}\\] The problem with the Split Half approach is that there are different ways to generate two halves, leading to different estimates of the reliability coefficient. To overcome this problem, we will assume that all items generate scores in tests of the same length (n = 1). These reliability estimates are based on the covariance of the items. 14.4.2 Item covariance tests 14.4.2.1 Cronbach's Alpha (\\(\\alpha\\)) The most famous reliability estimator based on the covariance of the items of a scale is Cronbach's alpha (\\(\\alpha\\)). Alpha is the mean of all possible split half reliabilities but corrected for the test length (Revelle &amp; Zinbarg, 2009). In Cronbach's alpha equation, \\(\\ n\\) is the number of items, \\(\\sigma^{2}_{i}\\) is the variance of the items, and \\(\\sigma^{2}_{x}\\) is the variance of the total test scores. \\[\\begin{aligned} \\alpha = \\frac{\\ n}{\\ n \\ - \\ 1} \\Bigg( 1 \\ - \\frac{\\displaystyle\\sum_{i=1}^{n} \\sigma^{2}_{i}} {\\sigma^{2}_{x}} \\Bigg) \\end{aligned}\\] Cronbach's alpha (\\(\\alpha\\)): Properties and limitations Alpha usually yields smaller values than Spearman-Brown's reliability coefficient. For that reason, alpha is considered Spearman-Brown coefficient's lower bound If the elements are parallel, alpha (\\(\\alpha\\)) yields the same value as Spearman-Brown's equation If we consider two parts of the same test, alpha is usually equivalent to the Rulon formula for Tau Equivalent tests The estimates of Cronbach's alpha and Spearman-Brown's reliability coefficients range from 0 to 1 Alpha is problematic if we find negative values Alpha is problematic when the factorial structure of a test is not normal Raw alpha is sensitive to differences in the item variances Do not estimate an overall Cronbach's alpha coefficient for a psychological test that is not unidimensional 14.4.2.2 Other reliability coefficients In addition to Cronbach's alpha (\\(\\alpha\\)), other coefficients of internal consistency have been proposed. For example, the Kuder-Richardson Formula 20 (KR-20) is suitable for dichotomous items. Another reliability estimation was proposed by Hoyt to compute alpha using a Repeated Measures Analysis of Variance (ANOVA) approximation, testing the main effects and interaction of respondents' scores and items' scores. Guttman also proposed six different coefficients. For example, Guttman's Lambda 3 (\\(\\lambda_{3}\\)) provides the same estimation as Cronbach's alpha. Likewise, Guttman's Lambda 4 (\\(\\lambda_{4}\\)) provides the same estimation as Spearman-Brown's reliability coefficient. Guttman's Lambda 6 (\\(\\lambda_{6}\\))—although is a better reliability estimator than Guttman's Lambdas 3 (\\(\\lambda_{3}\\)) and 4 (\\(\\lambda_{4}\\))—is sensitive to the number of items and the average intercorrelation of the items of the test. Last, Revelle's Beta coefficient (\\(\\beta\\)) provides the reliability estimation of the worst split half of a given scale. MacDonald's Omega coefficients (\\(\\omega_{h}\\), \\(\\omega_{t}\\)) are an appropriate approximation to the reliability of a test as they are based on hierarchical factor analysis. Omega estimates the reliability of general (\\(\\omega_{h}\\)) and total (\\(\\omega_{t}\\)) factor saturation. It is recommended to run an EFA before using the function omega() from the package psych . Omega hierarchical (\\(\\omega_{h}\\)) estimates the general factor saturation of a test. We can estimate this coefficient by running an Exploratory Factor Analysis (EFA) using the Schmid-Leiman transformation and an oblique rotation method. On the other hand, Omega total (\\(\\omega_{t}\\)) estimates the total reliability of the test and it is similar to Guttman's \\(\\lambda_{6}\\) (G6). It is important to note that Omega hierarchical (\\(\\omega_{h}\\)) is based upon the sum of the squared loadings on the general factor, whereas Omega total (\\(\\omega_{t}\\)) is based upon the sum of the squared loadings on all the factors. Estimations of reliability using Omega total (\\(\\omega_{t}\\)) produce similar results than estimations of reliability using Guttman's \\(\\lambda_{6}\\) (G6). "],["internal-consistency-in-r.html", "14.5 Internal consistency in R", " 14.5 Internal consistency in R In the following subsections, we will use the data included in the data set burnout.sav to estimate different measures of internal consistency. 14.5.1 Data wrangling Starting the session in R: Start every session in R by removing all objects from the global environment using the function rm(). Likewise, set in advance the randomization seed for the current session using the function set.seed() and avoid using the scientific notation. rm(list = ls()) set.seed(1234) options(scipen = 999) Setting the working directory: Place the data set burnout.sav in the same folder where you will set the working directory. Then, set the working directory accordingly. For example, the instructor's working directory can be found in a folder named Chapter14 that is located inside of the folder LDSwR and the folder R; all of them placed in the home folder Users. setwd(&#39;/Users/R/LDSwR/Chapter14&#39;) Loading packages: Load the packages that will be required to execute the R code for this session. library(rio) library(dplyr) library(psych) Reading the data set: To read the data from the SPSS file stored in the working directory, use the function import() from the package rio but do not forget to add the path with the additional folder (datasets) followed by the name of the data set with its corresponding file extension (e.g., .sav, .csv, .dat). burnout.dat &lt;- import(&#39;./datasets/burnout.sav&#39;) Subsetting: To generate the correlation or structure matrix (P), we only need the items of the Job Burnout Scale. However, this time we will use only the 10 items that were retained after computing the Exploratory Factor Analysis conducted in Chapter 13. Thus, we are going to subset the data set to select the items 1 to 12 using the function select() from the package dplyr and then drop the problematic items i4 and i5. The resulting 10 covariates will be stored in a new R object named burnout. burnout &lt;- burnout.dat %&gt;% select(i1:i12) %&gt;% select(-c(i4, i5)) headTail(burnout) ## i1 i2 i3 i6 i7 i8 i9 i10 i11 i12 ## 1 4 3 4 4 3 4 4 4 4 4 ## 2 2 2 4 2 3 2 3 3 4 3 ## 3 2 5 5 2 2 5 2 3 3 1 ## 4 3 3 2 3 3 2 3 4 3 3 ## ... ... ... ... ... ... ... ... ... ... ... ## 507 2 2 2 2 2 4 3 2 3 4 ## 508 1 3 5 1 3 4 3 4 1 2 ## 509 2 4 2 2 4 4 5 4 3 4 ## 510 4 4 4 2 4 2 5 4 5 2 dim(burnout) ## [1] 510 10 names(burnout) ## [1] &quot;i1&quot; &quot;i2&quot; &quot;i3&quot; &quot;i6&quot; &quot;i7&quot; &quot;i8&quot; &quot;i9&quot; &quot;i10&quot; &quot;i11&quot; &quot;i12&quot; Reverse scoring: We need to reverse the scoring of the two items that were negatively worded (i3 and i8). First, we will create a new vector (new) after selecting all the rows and only the columns of the items named i3 and i8. The vector named new is the result of subtracting a value of 6 to the scores of two items that range from 1 to 5. Consequently, a score of 1 will be transformed into a score of 5 (i.e., 6 - 1 = 5) and a score of 5 will be transformed into a score of 1 (i.e., 6 - 5 = 1). After reverse scoring the two items included in the vector new, we will substitute the original items i3 and i8 included in our data set with the new ones stored in the R object new. As usual, you can inspect the result using the function head(). new &lt;- 6 - burnout[ , c(&#39;i3&#39;, &#39;i8&#39;)] burnout[ , c(&#39;i3&#39;, &#39;i8&#39;)] &lt;- new head(burnout) ## i1 i2 i3 i6 i7 i8 i9 i10 i11 i12 ## 1 4 3 2 4 3 2 4 4 4 4 ## 2 2 2 2 2 3 4 3 3 4 3 ## 3 2 5 1 2 2 1 2 3 3 1 ## 4 3 3 4 3 3 4 3 4 3 3 ## 5 2 2 2 1 3 2 3 2 4 2 ## 6 3 3 2 3 3 3 3 3 4 2 "],["cronbachs-alpha-alpha-1.html", "14.6 Cronbach's alpha (\\(\\alpha\\))", " 14.6 Cronbach's alpha (\\(\\alpha\\)) First, we will use the function alpha() from the package psych to estimate Cronbach's alpha coefficient (\\(\\alpha\\)). Please, be aware that we are not considering the possibility of having two or more latent variables (i.e., factors). When using the function alpha() on all items, we are assuming that the scale is unidimensional (i.e., there is only one latent variable or factor). The function alpha() provides a lot of useful information. However, the relevant coefficients and sections to inspect are raw_alpha, G6(smc), and the raw_alpha column under the section reliability if an item is dropped. In the Job Burnout Scale, we found a marginally optimal Cronbach's reliability coefficient (\\(\\alpha = .78\\)). Although Guttman's \\(\\lambda_{6}\\) (G6) was also optimal (\\(\\lambda_{6} = .81\\)), be aware that it is usually considered an upper bound of Cronbach's alpha. The alpha coefficients displayed in the column raw_alpha inform us about what could happen to the alpha coefficient of our scale (\\(\\alpha = .78\\)) if we dropped the item under consideration. For example, the row related to item 11 (i.e., i11) shows that, if dropping that item, Cronbach's reliability coefficient (\\(\\alpha = .78\\)) might increase by 1% (\\(\\alpha = .79\\)). It is the only item of our scale that, if dropped, improves the reliability of our test. psych::alpha(burnout, check.keys = TRUE) ## ## Reliability analysis ## Call: psych::alpha(x = burnout, check.keys = TRUE) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.78 0.79 0.81 0.27 3.7 0.014 3.2 0.65 0.27 ## ## lower alpha upper 95% confidence boundaries ## 0.75 0.78 0.81 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## i1 0.75 0.75 0.78 0.25 3.0 0.017 0.023 0.27 ## i2 0.75 0.76 0.79 0.26 3.1 0.016 0.028 0.25 ## i3 0.77 0.77 0.80 0.27 3.3 0.016 0.029 0.25 ## i6 0.75 0.75 0.78 0.25 3.1 0.017 0.025 0.26 ## i7 0.76 0.77 0.78 0.27 3.3 0.016 0.023 0.28 ## i8 0.77 0.77 0.80 0.28 3.4 0.015 0.022 0.27 ## i9 0.76 0.76 0.77 0.26 3.1 0.016 0.024 0.27 ## i10 0.75 0.75 0.78 0.25 3.1 0.016 0.026 0.25 ## i11 0.79 0.80 0.81 0.30 3.9 0.014 0.018 0.28 ## i12 0.78 0.79 0.81 0.29 3.7 0.014 0.027 0.29 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## i1 510 0.68 0.68 0.65 0.57 2.7 1.12 ## i2 510 0.65 0.65 0.60 0.54 3.4 1.08 ## i3 510 0.61 0.58 0.51 0.45 2.9 1.36 ## i6 510 0.68 0.67 0.63 0.56 2.5 1.15 ## i7 510 0.59 0.60 0.57 0.47 3.3 1.11 ## i8 510 0.55 0.54 0.47 0.40 2.8 1.19 ## i9 510 0.63 0.64 0.62 0.52 3.6 1.03 ## i10 510 0.65 0.67 0.62 0.55 3.6 0.99 ## i11 510 0.36 0.37 0.26 0.20 3.7 1.12 ## i12 510 0.43 0.44 0.32 0.29 3.6 1.04 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## i1 0.12 0.36 0.23 0.23 0.06 0 ## i2 0.04 0.21 0.23 0.39 0.13 0 ## i3 0.17 0.30 0.12 0.25 0.15 0 ## i6 0.21 0.38 0.17 0.19 0.04 0 ## i7 0.05 0.19 0.25 0.35 0.15 0 ## i8 0.12 0.38 0.17 0.23 0.09 0 ## i9 0.04 0.11 0.24 0.42 0.20 0 ## i10 0.02 0.15 0.22 0.46 0.15 0 ## i11 0.05 0.12 0.17 0.40 0.26 0 ## i12 0.03 0.13 0.20 0.45 0.19 0 In the previous example, we used the function alpha() assuming that our scale was not unidimensional (i.e., all items reflect just one underlying latent variable or factor). However, we know that the burnout scale is not unidimensional, but bidimensional. We will create two datasets as a function of the EFA's factors previously extracted in the Chapter 13 (Exploratory Factor Analysis). Then, we will compute Cronbach's alpha per factor using the function alpha(). Please, note the difference of estimating Cronbach's alpha assuming unidimensionality or computing the same reliability coefficient of internal consistency when estimating alpha per factor. Estimating Cronbach's alpha on a non-unidimensional scale overestimates its reliability. burnout %&gt;% select(c(i7, i9, i11)) %&gt;% psych::alpha(check.keys = TRUE) ## ## Reliability analysis ## Call: psych::alpha(x = ., check.keys = TRUE) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.75 0.75 0.69 0.5 3 0.02 3.6 0.89 0.41 ## ## lower alpha upper 95% confidence boundaries ## 0.71 0.75 0.79 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## i7 0.58 0.58 0.41 0.41 1.4 0.037 NA 0.41 ## i9 0.58 0.58 0.41 0.41 1.4 0.037 NA 0.41 ## i11 0.81 0.81 0.68 0.68 4.2 0.017 NA 0.68 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## i7 510 0.85 0.85 0.77 0.64 3.3 1.1 ## i9 510 0.84 0.85 0.77 0.64 3.6 1.0 ## i11 510 0.75 0.74 0.50 0.45 3.7 1.1 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## i7 0.05 0.19 0.25 0.35 0.15 0 ## i9 0.04 0.11 0.24 0.42 0.20 0 ## i11 0.05 0.12 0.17 0.40 0.26 0 When we computed Cronbach's reliability coefficient for the whole test (\\(\\alpha = .78\\)), we concluded that the reliability was marginally optimal (\\(\\alpha &gt; .80\\) is usually optimal). A similar conclusion was reached with for Guttman's \\(\\lambda_{6}\\) (G6, \\(\\lambda_{6} = .81\\)). However, when estimating Cronbach's reliability coefficient for the three items reflecting factor 1 (Affective), the reliability of the test reflecting this latent variable dropped (\\(\\alpha = .75\\), \\(\\lambda_{6} = .69\\)). We also found that item 11 (i.e., i11) was a case of concern because dropping this item would have improved greatly the reliability of the scale (\\(\\alpha = .81\\)). Below, we will compute Cronbach's alpha for the seven items reflecting factor 2 (Reward). We will set the argument check.keys = TRUE to detect negatively correlated items and reverse score them automatically if needed. burnout %&gt;% select(-c(i7, i9, i11)) %&gt;% psych::alpha(check.keys = TRUE) ## ## Reliability analysis ## Call: psych::alpha(x = ., check.keys = TRUE) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.78 0.78 0.77 0.34 3.6 0.015 3.1 0.75 0.33 ## ## lower alpha upper 95% confidence boundaries ## 0.75 0.78 0.81 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## i1 0.73 0.73 0.71 0.31 2.7 0.019 0.0098 0.31 ## i2 0.74 0.75 0.74 0.33 3.0 0.017 0.0162 0.31 ## i3 0.77 0.77 0.76 0.36 3.4 0.015 0.0142 0.38 ## i6 0.74 0.74 0.72 0.33 2.9 0.018 0.0111 0.31 ## i8 0.75 0.75 0.74 0.34 3.1 0.017 0.0156 0.33 ## i10 0.74 0.74 0.73 0.32 2.8 0.018 0.0156 0.31 ## i12 0.79 0.79 0.78 0.39 3.9 0.015 0.0076 0.40 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## i1 510 0.75 0.76 0.73 0.63 2.7 1.12 ## i2 510 0.68 0.69 0.61 0.54 3.4 1.08 ## i3 510 0.62 0.58 0.47 0.41 2.9 1.36 ## i6 510 0.71 0.70 0.66 0.57 2.5 1.15 ## i8 510 0.68 0.67 0.59 0.52 2.8 1.19 ## i10 510 0.70 0.72 0.66 0.59 3.6 0.99 ## i12 510 0.47 0.49 0.35 0.30 3.6 1.04 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## i1 0.12 0.36 0.23 0.23 0.06 0 ## i2 0.04 0.21 0.23 0.39 0.13 0 ## i3 0.17 0.30 0.12 0.25 0.15 0 ## i6 0.21 0.38 0.17 0.19 0.04 0 ## i8 0.12 0.38 0.17 0.23 0.09 0 ## i10 0.02 0.15 0.22 0.46 0.15 0 ## i12 0.03 0.13 0.20 0.45 0.19 0 The previous results showed that Cronbach's reliability coefficient for the items reflecting factor 2 (Reward) was marginally optimal (\\(\\alpha = .78\\), \\(\\lambda_{6} = .77\\)). We found that, if we drop item 12 (i.e., i12), the reliability of the scale will be improved by 1% (\\(\\alpha = .79\\)). "],["other-reliability-coefficients-1.html", "14.7 Other reliability coefficients", " 14.7 Other reliability coefficients In this section, we will inspect other reliability coefficients generated with other functions that you can find in the package psych (Revelle, 2018; Revelle &amp; Zinbarg, 2009). Although most reliability coefficients will yield similar values despite the different functions being used, differences might arise due to the different procedures used to estimate these coefficients. In addition to reading the literature on the topic, read the description and details provided in every function that we use in R. First, we will use the function omega() to estimate both Omega hierarchical (\\(\\omega_{h}\\))---the estimation of the general factor saturation of a test---and Omega total (\\(\\omega_{t}\\))---the estimation of the total reliability of the test. We must set the optional argument nfactors to 2 because those were the number of factors that we extracted in Chapter 13 (Exploratory Factor Analysis). psych::omega(burnout, nfactors = 2, flip = TRUE) ## Omega ## Call: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, ## digits = digits, title = title, sl = sl, labels = labels, ## plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, ## covar = covar) ## Alpha: 0.79 ## G.6: 0.81 ## Omega Hierarchical: 0.41 ## Omega H asymptotic: 0.49 ## Omega Total 0.83 ## ## Schmid Leiman Factor loadings greater than 0.2 ## g F1* F2* h2 u2 p2 ## i1 0.43 0.62 0.57 0.43 0.33 ## i2 0.40 0.45 0.37 0.63 0.43 ## i3 0.36 0.31 0.26 0.74 0.49 ## i6 0.42 0.50 0.44 0.56 0.41 ## i7 0.47 0.66 0.66 0.34 0.34 ## i8 0.30 0.56 0.43 0.57 0.21 ## i9 0.50 0.65 0.68 0.32 0.37 ## i10 0.41 0.53 0.45 0.55 0.37 ## i11 0.24 0.45 0.28 0.72 0.20 ## i12 0.21 0.27 0.12 0.88 0.37 ## ## With eigenvalues of: ## g F1* F2* ## 1.5 1.6 1.1 ## ## general/max 0.92 max/min = 1.4 ## mean percent general = 0.35 with sd = 0.09 and cv of 0.26 ## Explained Common Variance of the general factor = 0.35 ## ## The degrees of freedom are 26 and the fit is 0.16 ## The number of observations was 510 with Chi Square = 82.82 with prob &lt; 0.000000077 ## The root mean square of the residuals is 0.04 ## The df corrected root mean square of the residuals is 0.05 ## RMSEA index = 0.065 and the 10 % confidence intervals are 0.05 0.082 ## BIC = -79.28 ## ## Compare this with the adequacy of just a general factor and no group factors ## The degrees of freedom for just the general factor are 35 and the fit is 1.32 ## The number of observations was 510 with Chi Square = 667.5 with prob &lt; 0.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000006 ## The root mean square of the residuals is 0.19 ## The df corrected root mean square of the residuals is 0.21 ## ## RMSEA index = 0.188 and the 10 % confidence intervals are 0.176 0.201 ## BIC = 449.29 ## ## Measures of factor score adequacy ## g F1* F2* ## Correlation of scores with factors 0.66 0.76 0.77 ## Multiple R square of scores with factors 0.43 0.58 0.59 ## Minimum correlation of factor score estimates -0.13 0.16 0.17 ## ## Total, General and Subset omega for each subset ## g F1* F2* ## Omega total for total scores and subscales 0.83 0.79 0.77 ## Omega general for total scores and subscales 0.41 0.30 0.25 ## Omega group for total scores and subscales 0.40 0.49 0.52 Omega hierarchical (\\(\\omega_{h}\\)) estimates the general factor saturation of a test. In the burnout data set, the factor loadings were moderate for some items (e.g., items i1 and i8) and low for others (e.g., items i11 and i12). For this reason, Omega hierarchical yields a low value (\\(\\omega_{h} = .41\\)). Be aware that the reliability coefficient estimated with Guttman's \\(\\lambda_{6}\\) (\\(G6 = .81\\)) tends to be similar to Omega total (\\(\\omega_{t} = .83\\)). Cronbach's alpha coefficient estimated with the function omega() is .79, whereas the coefficient estimated with the function alpha() yielded a lower value (\\(\\alpha = .78\\)). In case of disagreement, use Cronbach's alpha reliability coefficient estimated with the functions omega() or iclust() (in this example, \\(\\alpha = .79\\)). We can also inspect the tables (e.g., Schmid Leiman Factor loadings greater than 0.2) to reach the same conclusion in relation to keeping or dropping items. The decision to drop items has to consider the fact that latent variables with less than 3 observable measures (i.e., items) will lead to an unspecified model (Fabrigar et al., 1999). In the previous example, we produced a plot with the second order factor (g) displayed on the left. On the other hand, the first order factors (Affective and Reward; i.e., factors 1 and 2) were plotted on the right-hand side without an arrow connecting them (both factors were assumed to be uncorrelated or orthogonal). This model is called bifactor, but it is not easily found in psychology. As we discussed in previous sessions, most psychological constructs tend to be correlated to each other. Indeed, in the session Exploratory Factor Analysis we found that the two factors extracted from the Job Burnout Scale were correlated (interfactor correlation, \\(r = .37\\)). Consequently, we cannot use here a bifactor approach (i.e., factors are independent) to visualize the internal structure of the Job Burnout Scale, but a hierarchical approach. In the next visualization, we will be able to observe that the path diagram shows two first order latent variables (factors 1 and 2) and one second order factor (g). The argument sl refers to how we would like to plot the results (i.e., using the Schmid Leiman solution or not). The defaulted model assumes that the factors are independent (not correlated). The default option sets this argument to TRUE. Thus, by setting the argument sl to FALSE, we will plot a hierarchical solution, assuming that the first order latent variables are correlated and that they also reflect the higher order latent variable (g). my.omega &lt;- psych::omega(burnout, nfactors = 2, sl = FALSE, rotate = &#39;oblimin&#39;, fm = &#39;ml&#39;, digits = 2, option = &#39;equal&#39;, flip = TRUE) In the previous R code, we stored the computations executed with the function omega() in the object my.omega. We specified the number of factors (nfactors = 2) and the estimation of a hierarchical model (correlated factors; sl = FALSE). Likewise, we used the rotation method oblimin for correlated factors and the estimation method ml (maximum likelihood). For two factors, we had to specify equal loadings for both factors. Finally, we flipped the items negatively correlated to the general factor. The function omega.diagram() enables us to use the object my.omega to get the hierarchical plot with some interesting modifications. For example, we can set the labels of the factors (flabels = c('Affective', 'Reward')). We can also specify the cutoff point for the loadings (cut = .30), increase or decrease the font size (cex) or the size of the round boxes of the factors (e.size). omega.diagram(my.omega, sl = FALSE, flabels = c(&#39;Affective&#39;, &#39;Reward&#39;), cut = .3, digits = 2, e.size = .15, cex = 1.2, adj = 2, main = &#39; \\n Hierarchical Omega (Burnout dataset)&#39;) The function guttman() estimates Guttman's Lambdas (\\(\\lambda_{1}\\) to \\(\\lambda_{6}\\)). Please, remember that \\(\\lambda_{3}\\) provides the same reliability estimation as Cronbach's alpha coefficient (\\(\\alpha\\)). Beta (\\(\\beta\\)) is a reliability coefficient that looks for the worst split half to generate the coefficient of reliability (Revelle &amp; Zinbarg, 2009). Consequently, beta (\\(\\beta\\)) is an estimator that yields a low reliability value (i.e., a lower bound of reliability). However, the best estimation of beta (\\(\\beta\\)) is computed with the function iclust() that will be explained below. psych::guttman(burnout) ## Call: psych::guttman(r = burnout) ## ## Alternative estimates of reliability ## ## Guttman bounds ## L1 = 0.71 ## L2 = 0.8 ## L3 (alpha) = 0.79 ## L4 (max) = 0.85 ## L5 = 0.77 ## L6 (smc) = 0.81 ## TenBerge bounds ## mu0 = 0.79 mu1 = 0.8 mu2 = 0.8 mu3 = 0.8 ## ## alpha of first PC = 0.8 ## estimated greatest lower bound based upon communalities= 0.87 ## ## beta found by splitHalf = 0.6 The function splitHalf() computes the split half of the test and provides an estimation of \\(\\lambda_{3}\\), \\(\\lambda_{4}\\), and \\(\\lambda_{6}\\). The estimation of \\(\\beta\\) as the minimum split half reliability is important because it contrasts sharply with \\(\\lambda_{4}\\) (i.e., the greatest split half reliability). psych::splitHalf(burnout) ## Split half reliabilities ## Call: psych::splitHalf(r = burnout) ## ## Maximum split half reliability (lambda 4) = 0.85 ## Guttman lambda 6 = 0.81 ## Average split half reliability = 0.79 ## Guttman lambda 3 (alpha) = 0.79 ## Guttman lambda 2 = 0.8 ## Minimum split half reliability (beta) = 0.6 ## Average interitem r = 0.27 with median = 0.27 The function iclust() finds the overall correlation matrix to combine the two most similar items into a new (composite) item. Then, it computes the correlation of this new item with the remaining items and repeats this process until the worst split half correlation (\\(\\beta\\)) stops increasing. Here, beta (\\(\\beta\\)) will tend to coincide with omega hierarchical (\\(\\omega_{h}\\)) as both reliability coefficients estimate the general factor saturation (Revelle, 2018). In case of disagreements among the values generated for beta (\\(\\beta\\)) in other functions included in the package psych, always report iclust()'s original beta (\\(\\beta = .48\\)). psych::iclust(burnout, plot = TRUE, reverse = TRUE) ## ICLUST (Item Cluster Analysis) ## Call: psych::iclust(r.mat = burnout, reverse = TRUE, plot = TRUE) ## ## Purified Alpha: ## [1] 0.79 ## ## G6* reliability: ## [1] 1 ## ## Original Beta: ## [1] 0.48 ## ## Cluster size: ## [1] 10 ## ## Item by Cluster Structure matrix: ## [,1] ## i1 0.65 ## i2 0.60 ## i3 0.51 ## i6 0.63 ## i7 0.57 ## i8 0.47 ## i9 0.62 ## i10 0.62 ## i11 0.26 ## i12 0.32 ## ## With eigenvalues of: ## [1] 2.9 ## ## Purified scale intercorrelations ## reliabilities on diagonal ## correlations corrected for attenuation above diagonal: ## [,1] ## [1,] 0.79 ## ## Cluster fit = 0.65 Pattern fit = 0.93 RMSR = 0.12 "],["references-6.html", "14.8 References", " 14.8 References Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20, 37—46. Crocker, L., &amp; Algina, J. (1986). Introduction to classical and modern test theory. Holt, Rinehart, &amp; Winston. Cronbach, L. J. (1951). Coefficient alpha and the internal structure of tests. Psychometrika, 16, 297—334. DeVellis, R. F. (2017). Scale development: Theory and applications (4th ed.). SAGE. Fabrigar, L. R., Wegener, D. T., MacCallum, R. C., &amp; Strahan, E. J. (1999). Evaluating the use of exploratory factor analysis in psychological research. Psychological Methods, 4(3), 272—299. Friendly, M. (2017). Working with categorical data with R and the vcd and vcdExtra packages. Retrieved at http://cran.nexr.com/web/packages/vcdExtra/vignettes/vcd-tutorial.pdf Huysamen, G. K. (2006). Coefficient alpha: Unnecessarily ambiguous; Unduly ubiquitous. Journal of Industrial Psychology, 32, 34—40. Revelle, W. (2018). psych: Procedures for psychological, psychometric, and personality research. Retrieved at http://personality-project.org/r/psych/psych-manual.pdf Revelle, W., &amp; Zinbarg, R. E. (2009). Coefficients alpha, beta, omega, and the glb: Comments on Sijtsma. Psychometrika, 74(1), 145—154. Sijtsma, K. (2009). On the use, the misuse, and the very limited usefulness of Cronbach's alpha. Psychometrika, 74, 107—120. "],["introduction-to-item-response-theory.html", "Chapter 15 Introduction to Item Response Theory", " Chapter 15 Introduction to Item Response Theory LEARNING OUTCOMES Identify the key assumptions of Classical Test Theory (CTT) and Item Response Theory (IRT), critically evaluating CTT’s limitations. Distinguish the relevant features of the different IRT models for dichotomous and polytomous data. Explain the parameters bj, aj, and cj used in logistic models and appraise the visualizations displaying these parameters in Item Characteristic Curves (ICCs). Evaluate the reliability of items and tests by inspecting the Item Information Functions (IIFs) and the Test Information Functions (TIFs). In Chapter 12 (Scale development), we emphasized the importance of inspecting the psychometric quality of the scale once we have pre-tested our initial pool of items. To do so, we rely on four key psychometric principles: validity, reliability, fairness, and comparability (Mislevy, Wilson, Ercikan, &amp; Chudowsky, 2003). Guided by these principles, we evaluate the original scale and usually reduce the initial pool of items, retaining the best items that enhance the psychometric quality of the scale. After this quality control process, the remaining items are field tested with its corresponding iterative psychometric quality checks. Classical Test Theory (CTT) has been (and still is) profusely applied in scale development. However, in some fields (e.g., educational assessment) this psychometric model has become problematic, generating much needed alternatives taking the form of different models under the framework of Item Response Theory (IRT). Although the antecedents of IRT can be traced back to Thurstone's measurement model, its development and expansion flourished in the 1960s thanks to pioneers Frederic Lord, Melvin Novick, and Georg Rasch (Bock, 1997). Limitations of Classical Test Theory (CTT) CTT assumes a linear relationship between the latent variable and the observed scores. This linear trend is hardly observed in real life Test statistics (e.g., reliability, standard error of measurement) depend on the sample and population being measured. Global test statistics are sensible to the variability of the participants' responses CTT does not provide a theoretical model for the responses given to the items. We cannot evaluate the responses of one respondent as a function of the level of ability or trait for a given item. Consequently, the level of analysis in CTT is the overall test, not the individual items It is recommended to develop scales with a large number of items to sample the universe of items measuring a psychological construct. Consequently, larger scales are more reliable than shorter ones Item statistics (e.g., difficulty, discrimination) depend on the sample and population being measured. For example, item difficulty will be higher when the respondents are above the average. Likewise, discrimination indexes tend to be higher in samples with higher variability because they are estimated using correlation coefficients In CTT, tests' standard error of measurement is consistent across an entire sample or population. Thus, regardless of the raw test score (e.g., high, low), the standard error of measurement for each score will remain the same CTT is well suited for test scores produced by respondents with an average ability or trait. However, for extremes scores, CTT is problematic "],["item-response-theory-irt.html", "15.1 Item Response Theory (IRT)", " 15.1 Item Response Theory (IRT) Item Response Theory (IRT) refers to an array of structural models that establish a formalized mathematical relationship between the response to a specific item and the level of ability/trait of the respondent (Crocker &amp; Algina, 1986; DeVellis, 2017). IRT models assume that respondents' behaviors, when answering to one item, can be explained using one or more latent variables (i.e., unobserved traits or abilities). Consequently, respondents' scores in one ability test (e.g., a Spanish grammar test) will be a function of the underlying respondents' ability or trait (e.g., their knowledge of Spanish grammar). IRT focuses on the properties of individual items, rather than the overall scale. Interestingly, every respondent and item can be located at a certain point of the trait (\\(\\theta\\)). To do so, the scale has to be unidimensional (i.e., one ability/trait is enough to explain the results of the respondents and the relationships among the items). Despite this, later developments in test theory has allow for the development of multidimensional IRT (mIRT) models. IRT's item and test statistics (e.g., parameters, test information function and its standard error) differ greatly from Classical Test Theory statistics (e.g., item-test correlation, coefficient alpha). IRT allows us to overcome most of the problems that CTT faces (e.g., shorter tests can be more reliable than longer tests). As a trade-off, IRT is less flexible than CTT regarding the model's underlying assumptions. Likewise, IRT requires a large number of respondents in order to properly estimate the items' parameters and the Test Information Function (Table 15.1). The last interesting feature of IRT models regards to the score distributions. In CTT, interval scale properties are satisfied with normal score distributions and linearity assumptions, whereas in IRT the relationship between ability/trait level and the probability to endorse the item is monotonic (i.e., a non-linear increasing function that usually fits into exponential models). Table 15.1: Differences between Classical Test Theory and Item Response Theory Features CTT IRT Model Linear Non linear (monotonic) Level of analysis Test Item Relationship item-trait Not specified Item Characteristic Curve (ICC) Assumptions Weak Strong Item statistics Point-biserial correlation Parameters \\(a_{j}\\), \\(b_{j}\\), \\(c_{j}\\), and Information function Sample size N = 200 — 500 N &gt; 800 Note. CTT = Classical Test Theory. IRT = Item Response Theory. "],["item-characteristic-curve-icc.html", "15.2 Item Characteristic Curve (ICC)", " 15.2 Item Characteristic Curve (ICC) The probability of being correct on one item will be a monotonically increasing function of ability (\\(\\theta\\)), whereas the probability of endorsing an attitude item (i.e., polytomous item) will be a single peaked function of the level of that attitude (Revelle, 2023). The monotonically increasing function is called Item Characteristic Curve (ICC). It combines the proportion of correct responses in a given item (see \\(P(\\theta)\\) in the y-axis) with the ability/trait level (see \\(\\theta\\) in the x-axis). IRT's item statistics are the Item Characteristic Curve's parameters. These parameters depend on the IRT model being used. For example, a two-parameter logistic model (2PLM) will estimate the item's difficulty (\\(b_{j}\\)) and discrimination (\\(a_{j}\\)). 15.2.1 Example: Depression scores Let's imagine that we want to estimate the level of depression of 30 respondents using one dichotomous item (i.e., I do not enjoy going out with my friends anymore). Respondents may agree or disagree with the statement of this item. We assume that respondents agreeing with the previous statement are experiencing a major depressive episode (Table 15.2). Table 15.2: Responses to an Item as a Function of the Level of Depression Respondent Level of depression Agreement with the item 1 4 0 2 4 0 3 4 0 4 5 0 5 5 0 6 5 0 7 5 0 8 6 0 9 6 0 10 7 0 11 7 0 12 8 0 13 8 0 14 8 0 15 9 0 16 9 0 17 9 0 18 9 1 19 9 1 20 10 0 21 10 1 22 10 1 23 10 0 24 10 1 25 11 1 26 11 1 27 11 1 28 12 1 29 12 1 30 14 1 Note. Item: I do not enjoy going out with my friends anymore. If we plot the values displayed in Table 15.2, we will find that a monotonic distribution represents the probability of respondents agreeing with the item \"I do not enjoy going out with my friends anymore\" given their level of depression. Figure 15.2 shows that there is a 50% chance of agreeing with the item at levels of depression close to 10 (\\(\\theta\\) = 9.51). "],["irt-models.html", "15.3 IRT Models", " 15.3 IRT Models IRT models can be classified as a function of the trait being measured (ability versus personality/person-center), the response format (dichotomous versus polytomous items), and the number of parameters (1P versus 2P versus 3P...). Table 15.3 lists the most popular unidimensional IRT models. Table 15.3: Unidimensional Item Response Theory (IRT) models Item response Dichotomous Polytomous Rasch model (1P) Partial credit model (PCM) 1-Parameter logistic model (1PLM) Rating scale model (RSM) 2-Parameter logistic model (2PLM) Generalized partial credit model (GPCM) 3-Parameter logistic model (3PLM) Graded response model (GRM) 4-Parameter logistic model (4PLM) Nominal response model (NRM) It is beyond the scope of this chapter to cover non-parametric models (e.g., Mokken), 4PL or 5PL models (e.g., unfolding), complex polytomous models (e.g., GRM, NRM), or IRT multidimensional models for simple or bifactor structures (Ostini &amp; Nering, 2006; Paek &amp; Cole, 2020; Revelle, 2023). For this reason, only some of the most popular IRT models (i.e., 1PLM, 2PLM, 3PLM) will be briefly described in the following subsection. "],["parameters.html", "15.4 Parameters", " 15.4 Parameters The most common parameters used in logistic models are item difficulty (\\(b_{j}\\)), discrimination (\\(a_{j}\\)), and guessing (\\(c_{j}\\)). In addition to the values estimated for every parameter, IRT models allow us to graphically inspect these parameters using the Item Characteristic Curve (ICC). ICC plots place the location of respondents' level of ability/trait in the x-axis and the probability of endorsing the item in the y-axis. 15.4.1 Item difficulty (\\(b_{j}\\)) Item difficulty (\\(b_{j}\\)) shows the location of the item along the continuum of ability/trait (\\(\\theta_{i}\\)). Thus, \\(b_{j}\\) allows us to estimate the difficulty of every item. Items endorsed or correctly answered with a probability of 0.50 corresponding to high values of \\(\\theta_{i}\\) should be considered difficult items. In other words, an endorsement of 50% in a given item requiring high levels of ability means that the item is very difficult. 15.4.2 Item discrimination (\\(a_{j}\\)) Item discrimination (\\(a_{j}\\)) refers to the ability of an item to discriminate (i.e., differentiate) among respondents with different levels of ability/trait (\\(\\theta_{i}\\)). Consequently, items with larger discrimination values (e.g., \\(a_{j} = 6\\)) show larger slopes. In all 1 Parameter IRT models, \\(a_{j}\\) is fixed for all items. Thus, the discrimination parameter is set to 1 in the 1P Rasch model, whereas it is estimated from the data in the 1PLM. 15.4.3 Guessing (\\(c_{j}\\)) The guessing or pseudo-chance parameter (\\(c_{j}\\)) estimates the likelihood that a given respondent will select the right answer (i.e., ability tests) due to chance. "],["the-information-function.html", "15.5 The Information Function", " 15.5 The Information Function In psychometrics, the concept of information is related to the idea of precision with which a given ability/trait level (\\(\\theta\\)) can be estimated. The amount of information (\\(I\\)) is inversely related to the size of the variability of the estimates around the value of a parameter (\\(\\sigma^{2}\\)) (Baker &amp; Kim, 2017). \\[\\\\I\\ = \\frac{1}{\\sigma^{2}}\\] Thus, if the amount of information is small, the ability/trait cannot be estimated with precision (i.e., there will be a large variability of the estimates around the 'true' value of the parameter). In sum, the information function will show us how well each ability/trait level (\\(\\theta\\)) is being estimated. 15.5.1 Item Information Function Table 15.4 shows the parameter estimates computed on the 5-item test LSAT6 (Law School Admission Test, section 6) using a 2PL model (Chalmers, 2012; Thissen, 1982). Consequently, the additional parameters computed in 3PL (i.e., guessing) and 4PL (i.e., innatention) models were automatically constrained to 0 (i.e., no guessing) and 1 (i.e., no innatention) respectively. Table 15.4 shows that item 3 is the most difficult item to answer (\\(b_{j} = -0.28\\)) and the one that discriminates the most (\\(a_{j} = 0.89\\)) between respondents with abilities above and below \\(b_{j} = -0.28\\). In contrast, item 5 is one of the easiest items of the test (\\(b_{j} = -3.12\\)) and it is also the least discriminant item (\\(a_{j} = 0.66\\)). Table 15.4: Parameter estimation of the LSAT6 dataset using a 2PL model Parameters Item \\(a_{j}\\) \\(b_{j}\\) \\(c_{j}\\) \\(u_{j}\\) i1 0.83 -3.36 0 1 i2 0.72 -1.37 0 1 i3 0.89 -0.28 0 1 i4 0.69 -1.87 0 1 i5 0.66 -3.12 0 1 Note. \\(a_{j}\\) = Discrimination. \\(b_{j}\\) = Difficulty. \\(c_{j}\\) = Guessing. \\(u_{j}\\) = Inattention. Based on the estimated parameters for every item of a given test, we can compute and plot the amount of information (i.e., the precision to measure an ability level) provided by every item. It is important to note that the amount of information will have its peak on the item's difficulty parameter (\\(b_{j}\\)), decreasing the amount of information that the item provides with the departure from the \\(b_{j}\\) estimate. Relying on the LSAT6 parameter estimates (see Table 4), we can compute the Item Information Function (IIF) for all items using the following equation: \\[\\\\I_{j}(\\theta)\\ = \\ a^{2}_{j}\\ P_{j}(\\theta)\\ Q_{j}(\\theta)\\] The previous equation is suited for 2PLM. For 1PLM, we will fix the discrimination parameter (\\(a_{j} = 1\\)) in order to simplify the equation. \\[\\\\I_{j}(\\theta)\\ = \\ P_{j}(\\theta)\\ Q_{j}(\\theta)\\] We can also plot the Item Information Function (IIF) of individual items separately or plot the item information tracelines of several items into a single graphic. In the following plot, we can observe that item 3 is highly informative; its peak is close to \\(I_{j}(\\theta) = 0.20\\). Be aware that item 3 is highly informative around the ability levels above and below zero (\\(b_{j} = -0.28\\)). However, item 1 will be a better candidate than item 3 if we require an informative item for lower levels of ability (e.g., \\(\\theta = -3\\)). 15.5.2 Test Information Function The Test Information Function (TIF; \\(I(\\theta)\\)) can be defined as the sum of the item information functions of a test. As it is a function, the information (i.e., precision) of a test varies with every value of \\(\\theta\\). \\[\\\\I(\\theta)\\ = \\ \\displaystyle\\sum_{j=1}^{J}\\ I_{j}(\\theta)\\] The value that the TIF yields depends on: (1) the number of items, (2) the parameters \\(a_{j}\\) (i.e., discrimination) and \\(c_{j}\\) (i.e., guessing), and (3) the proximity between \\(\\theta\\) and \\(b_{j}\\). It is important to consider that the concept of Test Information cannot be fully understood without its reciprocal: the Standard Error of the estimate of the ability/trait (\\(SE(\\theta)\\)). \\[\\\\SE(\\theta)\\ = \\ \\frac{1}{\\sqrt{I(\\theta)}}\\] A test would be best for estimating the ability/trait of the respondents whose \\(\\theta\\) scores fall near the peak of the Test Information Function. Precision of the estimates will decrease when departing from that peak. However, the precision of the test won't be appropriate for \\(\\theta\\) values located below and above the thresholds in which the tracelines of \\(I(\\theta)\\) and \\(SE(\\theta)\\) converge. "],["references-7.html", "15.6 References", " 15.6 References Andrich, D. (1988). Rasch models for measurement. SAGE. Baker, F. B., &amp; Kim, S. -H. (2017). The basics of Item Response Theory using R. Springer. Bock, R. D. (1997). A brief history of item response theory. Educational Measurement: Issues and Practice, 16, 21—33. Chalmers R. P. (2012). mirt: A Multidimensional Item Response Theory Package for the R environment. Journal of Statistical Software, 48(6), 1—29. Crocker, L., &amp; Algina, J. (1986). Introduction to classical and modern test theory. Holt, Rinehart, &amp; Winston. DeVellis, R. F. (2017). Scale development: Theory and applications (4th ed.). SAGE. Mislevy, R. J., Wilson, M. R., Ercikan, K., &amp; Chudowsky, N. (2003). Psychometric principles in student assessment. In D. Stufflebeam &amp; T. Kellagham (Eds.), International handbook of educational evaluation (pp. 489—532). Kluwer Academic Press. Muraki, E. (1992). A generalized partial credit model: Application of an EM algorithm. Applied Psychological Measurement, 14(1), 59—71. Ostini, R., &amp; Nering, M. L. (2006). Polytomous item response theory models. SAGE. Paek, I., &amp; Cole, K. (2020). Using R for Item Response Theory model applications. Routledge. Revelle, W. (2023). An introduction to psychometric theory with applications in R. Samejima, F. (1969). Estimation of latent ability using a response pattern of graded scores. Psychometric Society. Thissen, D. (1982). Marginal maximum likelihood estimation for the one-parameter logistic model. Psychometrika, 47, 175—186. "],["appenA.html", "A Software ", " A Software "],["appenA1.html", "A.1 Installation of R and RStudio", " A.1 Installation of R and RStudio kkfdslf kdlfkdslkf "],["appenA2.html", "A.2 Installation of R packages", " A.2 Installation of R packages Although there are different ways to install packages, we will describe two of them below: using RStudio's GUI and running a line of code. RStudio's GUI: First, we must click on the button Install in the package tab. Then, we need to enter the name of the package in the CRAN repository to install it locally on the library directory. It is important to enable the option install dependencies because many packages depend on the installation of others to be correctly loaded and work properly. Running a line of code: This is the recommended option. It can be done directly on the console. The function install.packages() uses the first argument of the function to install the designated package. The name of the package must be written in quotes and the argument dependencies should be set to TRUE (or T). We can combine several packages with the function c() and install them at the same time. install.packages(&#39;binom&#39;, dependencies = TRUE) install.packages(c(&#39;tidyverse&#39;, &#39;car&#39;, &#39;lavaan&#39;, &#39;rio&#39;, &#39;magrittr&#39;, &#39;psych&#39;, &#39;likert&#39;), dependencies = TRUE) Some packages include a lot of functions and/or require the installation of several dependencies. If this is the case, expect a long installation process. Always check that the prompt (&gt;) symbol is displayed on the console as a confirmation that the installation process has ended. The information provided on the console during the installation process informs you of potential problems that were encountered when installing the target packages and their dependencies. For example, you may end up experiencing a very common problem when reading the following warning message on the console: Warning in install.packages : installation of package ‘ZZZ’ had non-zero exit status The above warning message informs about the need to install or to update (i.e., reinstall) the ZZZ package because R couldn't install it. Restart your R session and install the latest version of the package. It is convenient to update from time to time the packages previously installed. R is a dynamic, collaborative, and open-source computing environment. Thus, R is constantly evolving while being updated by the developers of the packages. The function update.packages() will update all the packages already installed on our computer. update.packages() "],["appenA3.html", "A.3 Packages used in this book", " A.3 Packages used in this book fewkfklew kewlfklew fewkfklew kewlfklew "],["appenB.html", "B Stylistic Conventions", " B Stylistic Conventions kjgkjdfg kgldskglkg sdklgdslg kldsglj "],["appenC.html", "C Universal Design for Learning", " C Universal Design for Learning fsdfdsfds fdsfsdf fdsfdsfs "]]
